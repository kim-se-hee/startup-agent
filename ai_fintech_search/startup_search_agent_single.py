# ai_fintech_search_single.py
"""
AI í•€í…Œí¬ ìŠ¤íƒ€íŠ¸ì—… ê²€ìƒ‰ ì—ì´ì „íŠ¸ (ë‹¨ì¼ íŒŒì¼ ë²„ì „)
ì‚¬ìš©ë²•: python ai_fintech_search_single.py --region Korea --limit 10
"""

import os
import argparse
import json
from typing import List, Annotated
from urllib.parse import urlparse, urlunparse
from dotenv import load_dotenv

from pydantic import BaseModel, Field, ConfigDict
from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_tavily import TavilySearch


# ============================================================================
# í™˜ê²½ ì„¤ì •
# ============================================================================
load_dotenv()

OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY", "")

def ensure_required_env():
    """í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜ í™•ì¸"""
    required = ["OPENAI_API_KEY", "TAVILY_API_KEY"]
    missing = [k for k in required if not os.getenv(k)]
    if missing:
        lines = [f"- {k} ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤." for k in missing]
        msg = "ë‹¤ìŒ í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤:\n" + "\n".join(lines) + "\n(.envì— í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”)"
        raise RuntimeError(msg)

ensure_required_env()


# ============================================================================
# ë°ì´í„° ëª¨ë¸
# ============================================================================
class StartupHit(BaseModel):
    """ìŠ¤íƒ€íŠ¸ì—… ê²€ìƒ‰ ê²°ê³¼"""
    name: str
    domain: str
    description: str

    model_config = ConfigDict(
        str_strip_whitespace=True,
    )


class StartupSearchResult(BaseModel):
    """LLM êµ¬ì¡°í™” ì¶œë ¥ìš©"""
    items: List[StartupHit] = []


class SearchState(BaseModel):
    """ì—ì´ì „íŠ¸ ìƒíƒœ"""
    messages: Annotated[list, add_messages] = []
    region: str = "Global"
    limit: int = 12
    language: str = "ko"
    results: List[StartupHit] = []
    
    model_config = ConfigDict(arbitrary_types_allowed=True)


# ============================================================================
# ìœ í‹¸ë¦¬í‹°
# ============================================================================
def normalize_root_url(url: str) -> str:
    """URL ì •ê·œí™”"""
    try:
        u = urlparse(url)
        clean = u._replace(path="/", params="", query="", fragment="")
        return urlunparse(clean)
    except Exception:
        return url


# ============================================================================
# ë„êµ¬ (Tools)
# ============================================================================
_tavily = TavilySearch(
    max_results=30,
    api_key=TAVILY_API_KEY
)

def web_search(query: str) -> list:
    """ì›¹ ê²€ìƒ‰"""
    return _tavily.invoke({"query": query})


# ============================================================================
# í”„ë¡¬í”„íŠ¸
# ============================================================================
SYSTEM_PROMPT = """ë‹¹ì‹ ì€ í•œêµ­ ìŠ¤íƒ€íŠ¸ì—… ì „ë¬¸ ë¦¬ì„œì²˜ì…ë‹ˆë‹¤.
ëª©í‘œ: 2025ë…„ ê¸°ì¤€, 'í•œêµ­ì˜ AI ê¸°ìˆ ì„ ì‚¬ìš©í•˜ëŠ” í•€í…Œí¬(ê¸ˆìœµ) ìŠ¤íƒ€íŠ¸ì—…'ë§Œ ì¶”ë ¤
'name, domain, description' í•„ë“œë¡œ êµ¬ì„±ëœ JSONì„ ë°˜í™˜í•©ë‹ˆë‹¤.

ê·œì¹™:
- **í•œêµ­ ê¸°ì—…ë§Œ ì„ ì •**: í•œêµ­ì— ë³¸ì‚¬ë¥¼ ë‘” ìŠ¤íƒ€íŠ¸ì—…ë§Œ í¬í•¨. í•´ì™¸ ê¸°ì—…ì€ ì œì™¸.
- ë°˜ë“œì‹œ 'AI ê¸°ìˆ ì„ í•µì‹¬ì— í™œìš©'í•˜ëŠ” í•€í…Œí¬ì¼ ê²ƒ(LLM/RAG/ML/NLP/CV/ì¶”ì²œ/ë¦¬ìŠ¤í¬ëª¨ë¸ ë“±).
- ì€í–‰/ëŒ€ê¸°ì—…ì˜ ì‚¬ì—…ë¶€ë‚˜ BaaS ë²¤ë”ëŠ” ì œì™¸í•˜ê³  'ìŠ¤íƒ€íŠ¸ì—…' ì¤‘ì‹¬.
- ë¸”ë¡ì²´ì¸/ê°€ìƒìì‚°ì€ 'í•€í…Œí¬'ë¡œ ê°„ì£¼ ê°€ëŠ¥í•˜ë‚˜, AI í•µì‹¬ í™œìš©ì´ í™•ì¸ë¼ì•¼ í¬í•¨.
- 2025ë…„ ë§¥ë½(ìµœê·¼ ê¸°ì‚¬/ê³µì‹ ì†Œê°œ)ì„ ìš°ì„  ë°˜ì˜. ì˜¤ë˜ëœ/ë¹„í™œì„± íšŒì‚¬ ì œì™¸.
- íšŒì‚¬ë‹¹ 1~2ë¬¸ì¥ìœ¼ë¡œ descriptionì„ ì‘ì„±(í•œêµ­ì–´).
- domainì€ 'Fintech/ì„¸ë¶€ë¶„ì•¼' í˜•íƒœë¡œ ê°„ê²°í•˜ê²Œ í‘œê¸°(ì˜ˆ: 'Fintech/ì‹ ìš©í‰ê°€', 'Fintech/ê²°ì œ').
- ì¤‘ë³µ/ë™ì¼ íšŒì‚¬ ì œê±°, ìµœëŒ€ {limit}ê°œ.
- ìµœì¢… ì¶œë ¥ì€ JSONë§Œ(ë¬¸ì¥Â·í•´ì„¤ ê¸ˆì§€), ìŠ¤í‚¤ë§ˆ:
  {{"items":[{{"name":"","domain":"","description":""}}, ...]}}
"""

USER_QUERY_TMPL = """ì•„ë˜ëŠ” ì›¹ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤. 2025ë…„ ê¸°ì¤€ìœ¼ë¡œ ìœ íš¨í•œ 'í•œêµ­ AI í•€í…Œí¬ ìŠ¤íƒ€íŠ¸ì—…'ë§Œ ì¶”ë ¤ ì£¼ì„¸ìš”.
í•´ì™¸ ê¸°ì—…ì€ ì œì™¸í•˜ê³ , í•œêµ­ ê¸°ì—…ë§Œ ì„ ì •í•˜ì„¸ìš”.
í•œêµ­ì–´ë¡œ ìš”ì•½í•˜ë©°, ì§€ì • ìŠ¤í‚¤ë§ˆ(JSON-only)ë¡œë§Œ ë‹µí•˜ì„¸ìš”.

ê²€ìƒ‰ ì§ˆì˜:
{query}

ê²€ìƒ‰ ê²°ê³¼(ìµœëŒ€ 30ê°œ):
{results}
"""


# ============================================================================
# ì—ì´ì „íŠ¸
# ============================================================================
_llm = ChatOpenAI(model=OPENAI_MODEL, temperature=0.1)


def _startup_search_node(state: SearchState) -> SearchState:
    """ìŠ¤íƒ€íŠ¸ì—… ê²€ìƒ‰ ë…¸ë“œ"""
    # í•œêµ­ ìŠ¤íƒ€íŠ¸ì—…ì— ì§‘ì¤‘
    base_terms = [
        "í•œêµ­ AI ì‹ ìš©í‰ê°€ ìŠ¤íƒ€íŠ¸ì—…",
        "êµ­ë‚´ ë¡œë³´ì–´ë“œë°”ì´ì € ìŠ¤íƒ€íŠ¸ì—…",
        "í•œêµ­ AI ëŒ€ì¶œ í•€í…Œí¬",
        "êµ­ë‚´ ì´ìƒê±°ë˜íƒì§€ FDS AI",
        "í•œêµ­ í•€í…Œí¬ ìŠ¤íƒ€íŠ¸ì—… íˆ¬ììœ ì¹˜",
        "êµ­ë‚´ ê¸ˆìœµ AI ìŠ¤íƒ€íŠ¸ì—… ì‹œë¦¬ì¦ˆ"
    ]
    
    if state.region.lower() == "korea":
        base_terms += [
            "í¬ë ˆíŒŒìŠ¤ ë±…í¬ìƒëŸ¬ë“œ í† ìŠ¤",
            "í•€í…Œí¬ ìŠ¤íƒ€íŠ¸ì—… AI ê¸°ìˆ  í™œìš©",
            "ê¸ˆìœµ ì¸ê³µì§€ëŠ¥ ìŠ¤íƒ€íŠ¸ì—… ì‚¬ë¡€"
        ]
    else:
        base_terms += [
            "Korean fintech AI companies",
            "South Korea fintech artificial intelligence"
        ]

    query = " OR ".join(base_terms)
    search_results = web_search(query)

    sys = SYSTEM_PROMPT.format(limit=state.limit)
    user = USER_QUERY_TMPL.format(query=query, results=search_results)

    structured = _llm.with_structured_output(StartupSearchResult)
    out: StartupSearchResult = structured.invoke([
        SystemMessage(content=sys),
        HumanMessage(content=user)
    ])

    uniq = {}
    cleaned: List[StartupHit] = []
    for item in out.items:
        key = item.name.strip().lower()
        if key in uniq:
            continue
        uniq[key] = True
        cleaned.append(StartupHit(
            name=item.name.strip(),
            domain=item.domain.strip(),
            description=item.description.strip()
        ))
    state.results = cleaned[:state.limit]
    return state


def build_graph():
    """ê·¸ë˜í”„ ë¹Œë“œ"""
    graph = StateGraph(SearchState)
    graph.add_node("startup_search", _startup_search_node)
    graph.set_entry_point("startup_search")
    graph.set_finish_point("startup_search")
    return graph.compile(checkpointer=MemorySaver())


def run_startup_search(region: str = "Global", limit: int = 12, language: str = "ko") -> List[StartupHit]:
    """ìŠ¤íƒ€íŠ¸ì—… ê²€ìƒ‰ ì‹¤í–‰"""
    app = build_graph()
    init = SearchState(region=region, limit=limit, language=language)
    config = {"configurable": {"thread_id": "startup_search_session"}}
    out = app.invoke(init, config)
    return out["results"]


# ============================================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================================
def main():
    p = argparse.ArgumentParser(description="AI Fintech Startup Finder (2025)")
    p.add_argument("--region", default="Korea", help="Global | Korea")
    p.add_argument("--limit", type=int, default=10, help="ê²€ìƒ‰í•  ìŠ¤íƒ€íŠ¸ì—… ìˆ˜")
    args = p.parse_args()

    print(f"ğŸ” í•œêµ­ AI í•€í…Œí¬ ìŠ¤íƒ€íŠ¸ì—… ê²€ìƒ‰ ì¤‘... (ìµœëŒ€ {args.limit}ê°œ)\n")
    
    hits = run_startup_search(region=args.region, limit=args.limit, language="ko")
    
    output = {"items": [h.model_dump() for h in hits]}
    print(json.dumps(output, ensure_ascii=False, indent=2))
    
    print(f"\nâœ… ì´ {len(hits)}ê°œì˜ ìŠ¤íƒ€íŠ¸ì—…ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()