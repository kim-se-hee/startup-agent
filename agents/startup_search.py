"""
Module: agents/startup_search.py (LangGraph + í•œêµ­ AI í•€í…Œí¬ íŠ¹í™” ë²„ì „)
Purpose: "ìŠ¤íƒ€íŠ¸ì—… íƒìƒ‰" ì—ì´ì „íŠ¸ (LLM Structured Output ê¸°ë°˜) â€” LangGraph StateGraph

íŠ¹ì§•:
- í•œêµ­ AI í•€í…Œí¬ ìŠ¤íƒ€íŠ¸ì—… íŠ¹í™” ê²€ìƒ‰
- LLM Structured Outputìœ¼ë¡œ ì •í™•í•œ ë°ì´í„° ì¶”ì¶œ
- Pydantic ìŠ¤í‚¤ë§ˆ ê¸°ë°˜ ê²€ì¦
- ì¤‘ë³µ ì œê±° ë° ê´€ë ¨ë„ ìŠ¤ì½”ì–´ë§

ì…ë ¥ State ì˜ˆì‹œ:
state = {
    "segment": "fintech ai",        # (í•„ìˆ˜) íƒìƒ‰ ë„ë©”ì¸/í‚¤ì›Œë“œ
    "region": "Korea",              # (ì„ íƒ) ì§€ì—­ (Korea|Global)
    "limit": 10,                    # (ì„ íƒ) í›„ë³´ ìˆ˜ ìƒí•œ
    "language": "ko"                # (ì„ íƒ) ì–¸ì–´
}

ì¶œë ¥ ë³‘í•© ì˜ˆì‹œ:
state["startup_search"] = {
  "query": "...",
  "candidates": [
      {"name": "í† ìŠ¤", "domain": "Fintech/ê²°ì œ", "description": "...", "url": "...", "score": 0.95},
      ...
  ],
  "docs": [...],
  "ts": 1690000000
}

í™˜ê²½ ë³€ìˆ˜:
- OPENAI_API_KEY (í•„ìˆ˜)
- TAVILY_API_KEY (í•„ìˆ˜)
"""
from __future__ import annotations

import os
import re
import time
import hashlib
import json
from dataclasses import dataclass
from typing import List, Dict, Any, Optional, TypedDict, Annotated
from urllib.parse import urlparse, urlunparse
from dotenv import load_dotenv

# .env ë¡œë“œ
load_dotenv()

# LangGraph
try:
    from langgraph.graph import StateGraph, START, END
    LANGGRAPH_AVAILABLE = True
except Exception:
    LANGGRAPH_AVAILABLE = False

# LangChain (Structured Outputìš©)
try:
    from langchain_openai import ChatOpenAI
    from langchain_core.messages import SystemMessage, HumanMessage
    LANGCHAIN_AVAILABLE = True
except Exception:
    LANGCHAIN_AVAILABLE = False

# Pydantic (ìŠ¤í‚¤ë§ˆ)
try:
    from pydantic import BaseModel, Field, ConfigDict
    PYDANTIC_AVAILABLE = True
except Exception:
    PYDANTIC_AVAILABLE = False

# Tavily (í•„ìˆ˜)
try:
    from tavily import TavilyClient
    TAVILY_AVAILABLE = True
except Exception as e:
    TAVILY_AVAILABLE = False
    print(f"âš ï¸ Tavily import ì‹¤íŒ¨: {e}")

# ----------------------------- Pydantic Models (from single file) -----------------------------

if PYDANTIC_AVAILABLE:
    class StartupHit(BaseModel):
        """ìŠ¤íƒ€íŠ¸ì—… ê²€ìƒ‰ ê²°ê³¼"""
        name: str
        domain: str
        description: str
        url: Optional[str] = ""
        score: Optional[float] = 0.0
        
        model_config = ConfigDict(str_strip_whitespace=True)
    
    class StartupSearchResult(BaseModel):
        """LLM êµ¬ì¡°í™” ì¶œë ¥ìš©"""
        items: List[StartupHit] = []
else:
    StartupHit = dict
    StartupSearchResult = dict


# ----------------------------- State & Config -----------------------------

class StartupSearchOutput(TypedDict, total=False):
    query: str
    candidates: List[Dict[str, Any]]
    docs: List[Dict[str, Any]]
    ts: int

class StartupSearchState(TypedDict, total=False):
    segment: str
    region: str
    limit: int
    language: str
    startup_search: StartupSearchOutput

@dataclass
class StartupSearchConfig:
    model: str = "gpt-4o-mini"
    temperature: float = 0.1
    max_results: int = 10
    include_content_chars: int = 1800
    use_structured_output: bool = True  # LLM structured output ì‚¬ìš© ì—¬ë¶€

# ----------------------------- Utilities -----------------------------

FINTECH_KEYWORDS = [
    "fintech", "payment", "payments", "lending", "credit", "scoring",
    "fraud", "risk", "insurance", "insurtech", "kyc", "aml", "banking",
]
AI_KEYWORDS = ["ai", "machine learning", "ml", "deep learning", "llm", "model"]

def _norm(text: str) -> str:
    return (text or "").strip()

def normalize_root_url(url: str) -> str:
    """URL ì •ê·œí™” (from single file)"""
    try:
        u = urlparse(url)
        clean = u._replace(path="/", params="", query="", fragment="")
        return urlunparse(clean)
    except Exception:
        return url

def _score(title: str, snippet: str) -> float:
    text = f"{title} {snippet}".lower()
    f_hits = sum(k in text for k in FINTECH_KEYWORDS)
    a_hits = sum(k in text for k in AI_KEYWORDS)
    return min(1.0, (0.6 * (f_hits/5.0)) + (0.4 * (a_hits/4.0)))

def _extract_company_from_title(title: str) -> str:
    t = title.split(" - ")[0].split(" | ")[0]
    t = re.sub(r"(News|TechCrunch|Crunchbase|Forbes|Bloomberg|Official Site)$", "", t, flags=re.I).strip()
    t = re.sub(r"(AI|Fintech|Startup|Company)$", "", t, flags=re.I).strip()
    return t if len(t) >= 2 else title.strip()

def _dedupe_by_url(items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    seen = set()
    out = []
    for it in items:
        key = hashlib.md5(_norm(it.get("url", "")).encode("utf-8")).hexdigest()
        if key in seen:
            continue
        seen.add(key)
        out.append(it)
    return out


# ----------------------------- Prompts (from single file) -----------------------------

SYSTEM_PROMPT = """ë‹¹ì‹ ì€ í•œêµ­ ìŠ¤íƒ€íŠ¸ì—… ì „ë¬¸ ë¦¬ì„œì²˜ì…ë‹ˆë‹¤.
ëª©í‘œ: 2025ë…„ ê¸°ì¤€, 'í•œêµ­ì˜ AI ê¸°ìˆ ì„ ì‚¬ìš©í•˜ëŠ” í•€í…Œí¬(ê¸ˆìœµ) ìŠ¤íƒ€íŠ¸ì—…'ë§Œ ì¶”ë ¤
'name, domain, description' í•„ë“œë¡œ êµ¬ì„±ëœ JSONì„ ë°˜í™˜í•©ë‹ˆë‹¤.

ê·œì¹™:
- **í•œêµ­ ê¸°ì—…ë§Œ ì„ ì •**: í•œêµ­ì— ë³¸ì‚¬ë¥¼ ë‘” ìŠ¤íƒ€íŠ¸ì—…ë§Œ í¬í•¨. í•´ì™¸ ê¸°ì—…ì€ ì œì™¸.
- ë°˜ë“œì‹œ 'AI ê¸°ìˆ ì„ í•µì‹¬ì— í™œìš©'í•˜ëŠ” í•€í…Œí¬ì¼ ê²ƒ(LLM/RAG/ML/NLP/CV/ì¶”ì²œ/ë¦¬ìŠ¤í¬ëª¨ë¸ ë“±).
- ì€í–‰/ëŒ€ê¸°ì—…ì˜ ì‚¬ì—…ë¶€ë‚˜ BaaS ë²¤ë”ëŠ” ì œì™¸í•˜ê³  'ìŠ¤íƒ€íŠ¸ì—…' ì¤‘ì‹¬.
- ë¸”ë¡ì²´ì¸/ê°€ìƒìì‚°ì€ 'í•€í…Œí¬'ë¡œ ê°„ì£¼ ê°€ëŠ¥í•˜ë‚˜, AI í•µì‹¬ í™œìš©ì´ í™•ì¸ë¼ì•¼ í¬í•¨.
- 2025ë…„ ë§¥ë½(ìµœê·¼ ê¸°ì‚¬/ê³µì‹ ì†Œê°œ)ì„ ìš°ì„  ë°˜ì˜. ì˜¤ë˜ëœ/ë¹„í™œì„± íšŒì‚¬ ì œì™¸.
- íšŒì‚¬ë‹¹ 1~2ë¬¸ì¥ìœ¼ë¡œ descriptionì„ ì‘ì„±(í•œêµ­ì–´).
- domainì€ 'Fintech/ì„¸ë¶€ë¶„ì•¼' í˜•íƒœë¡œ ê°„ê²°í•˜ê²Œ í‘œê¸°(ì˜ˆ: 'Fintech/ì‹ ìš©í‰ê°€', 'Fintech/ê²°ì œ').
- ì¤‘ë³µ/ë™ì¼ íšŒì‚¬ ì œê±°, ìµœëŒ€ {limit}ê°œ.
- ìµœì¢… ì¶œë ¥ì€ JSONë§Œ(ë¬¸ì¥Â·í•´ì„¤ ê¸ˆì§€), ìŠ¤í‚¤ë§ˆ:
  {{"items":[{{"name":"","domain":"","description":""}}, ...]}}
"""

USER_QUERY_TMPL = """ì•„ë˜ëŠ” ì›¹ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤. 2025ë…„ ê¸°ì¤€ìœ¼ë¡œ ìœ íš¨í•œ 'í•œêµ­ AI í•€í…Œí¬ ìŠ¤íƒ€íŠ¸ì—…'ë§Œ ì¶”ë ¤ ì£¼ì„¸ìš”.
í•´ì™¸ ê¸°ì—…ì€ ì œì™¸í•˜ê³ , í•œêµ­ ê¸°ì—…ë§Œ ì„ ì •í•˜ì„¸ìš”.
í•œêµ­ì–´ë¡œ ìš”ì•½í•˜ë©°, ì§€ì • ìŠ¤í‚¤ë§ˆ(JSON-only)ë¡œë§Œ ë‹µí•˜ì„¸ìš”.

ê²€ìƒ‰ ì§ˆì˜:
{query}

ê²€ìƒ‰ ê²°ê³¼(ìµœëŒ€ 30ê°œ):
{results}
"""

# ----------------------------- Node (LangGraph) -----------------------------

def _startup_search_node_factory(cfg: StartupSearchConfig):
    """LangGraph ë…¸ë“œ í•¨ìˆ˜ë¥¼ êµ¬ì„±í•˜ëŠ” íŒ©í† ë¦¬. cfgë¥¼ í´ë¡œì €ë¡œ ìº¡ì²˜."""
    def node(state: StartupSearchState) -> StartupSearchState:
        segment = _norm(state.get("segment", "fintech ai"))
        region = _norm(state.get("region", "Korea"))
        limit = int(state.get("limit", cfg.max_results))
        language = _norm(state.get("language", "ko"))

        # í•œêµ­ ìŠ¤íƒ€íŠ¸ì—…ì— íŠ¹í™”ëœ ê²€ìƒ‰ì–´ (from single file)
        if region.lower() == "korea":
            base_terms = [
                "í•œêµ­ AI ì‹ ìš©í‰ê°€ ìŠ¤íƒ€íŠ¸ì—…",
                "êµ­ë‚´ ë¡œë³´ì–´ë“œë°”ì´ì € ìŠ¤íƒ€íŠ¸ì—…",
                "í•œêµ­ AI ëŒ€ì¶œ í•€í…Œí¬",
                "êµ­ë‚´ ì´ìƒê±°ë˜íƒì§€ FDS AI",
                "í•œêµ­ í•€í…Œí¬ ìŠ¤íƒ€íŠ¸ì—… íˆ¬ììœ ì¹˜",
                "êµ­ë‚´ ê¸ˆìœµ AI ìŠ¤íƒ€íŠ¸ì—… ì‹œë¦¬ì¦ˆ",
                "í¬ë ˆíŒŒìŠ¤ ë±…í¬ìƒëŸ¬ë“œ í† ìŠ¤",
                "í•€í…Œí¬ ìŠ¤íƒ€íŠ¸ì—… AI ê¸°ìˆ  í™œìš©",
                "ê¸ˆìœµ ì¸ê³µì§€ëŠ¥ ìŠ¤íƒ€íŠ¸ì—… ì‚¬ë¡€"
            ]
        else:
            base_terms = [
                f"{segment} {region} startup",
                "Korean fintech AI companies",
                "South Korea fintech artificial intelligence"
            ]
        
        query = " OR ".join(base_terms)
        docs: List[Dict[str, Any]] = []
        search_results_raw = []

        # Tavily ê²€ìƒ‰
        tavily_key = os.getenv("TAVILY_API_KEY")
        tavily_success = False
        
        if TAVILY_AVAILABLE and tavily_key:
            try:
                print(f"ğŸ” Tavily ê²€ìƒ‰ ì‹œì‘: {base_terms[0]}")
                client = TavilyClient(api_key=tavily_key)
                
                # ì—¬ëŸ¬ ì¿¼ë¦¬ë¡œ ë‚˜ëˆ ì„œ ê²€ìƒ‰ (ë” ë§ì€ ê²°ê³¼ í™•ë³´)
                for term in base_terms[:2]:  # ì²˜ìŒ 2ê°œ ê²€ìƒ‰ì–´ë§Œ ì‚¬ìš©
                    try:
                        res = client.search(
                            query=term,
                            max_results=min(limit * 2, 20),
                            search_depth="advanced"  # ë” ê¹Šì€ ê²€ìƒ‰
                        )
                        
                        results = res.get("results", [])
                        print(f"   âœ… '{term}': {len(results)}ê°œ ê²°ê³¼ ë°œê²¬")
                        
                        for r in results:
                            doc = {
                                "source": "tavily",
                                "url": _norm(r.get("url")),
                                "title": _norm(r.get("title")),
                                "snippet": _norm(r.get("content", ""))[:240],
                                "content": _norm(r.get("content", ""))[: cfg.include_content_chars],
                            }
                            docs.append(doc)
                            search_results_raw.append(doc)
                        
                        if docs:
                            tavily_success = True
                            
                    except Exception as e:
                        print(f"   âš ï¸ '{term}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                        continue
                
            except Exception as e:
                print(f"âš ï¸ Tavily í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        else:
            if not tavily_key:
                print(f"âš ï¸ TAVILY_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            if not TAVILY_AVAILABLE:
                print(f"âš ï¸ Tavily ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (pip install tavily-python)")

        # Tavily ì‹¤íŒ¨ ì‹œ ë” ë‹¤ì–‘í•œ placeholder ë°ì´í„° ì œê³µ
        if not docs:
            print(f"âš ï¸ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ - ê¸°ë³¸ í•œêµ­ AI í•€í…Œí¬ ë°ì´í„° ì‚¬ìš©")
            docs = [
                {
                    "source": "placeholder",
                    "url": "https://toss.im",
                    "title": "í† ìŠ¤ - AI ê¸°ë°˜ ê¸ˆìœµ ìŠˆí¼ì•±",
                    "snippet": "ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì‹ ìš©í‰ê°€, ë§ì¶¤í˜• ê¸ˆìœµìƒí’ˆ ì¶”ì²œ, ì‚¬ê¸°íƒì§€ ì‹œìŠ¤í…œ",
                    "content": "í† ìŠ¤ëŠ” AI/ML ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ ì‹ ìš©í‰ê°€, ê°œì¸í™” ì¶”ì²œ, ì‚¬ê¸°íƒì§€ ë“± ë‹¤ì–‘í•œ ê¸ˆìœµ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” í•œêµ­ì˜ ëŒ€í‘œ í•€í…Œí¬ ê¸°ì—…ì…ë‹ˆë‹¤.",
                },
                {
                    "source": "placeholder",
                    "url": "https://www.banksalad.com",
                    "title": "ë±…í¬ìƒëŸ¬ë“œ - AI ìì‚°ê´€ë¦¬ í”Œë«í¼",
                    "snippet": "AI ê¸°ë°˜ ìì‚°ê´€ë¦¬, ê¸ˆìœµìƒí’ˆ ë¹„êµ ì¶”ì²œ, ê°œì¸í™”ëœ ì¬ë¬´ ë¶„ì„",
                    "content": "ë±…í¬ìƒëŸ¬ë“œëŠ” AI ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ê¸ˆìœµ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ë§ì¶¤í˜• ìì‚°ê´€ë¦¬ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.",
                },
                {
                    "source": "placeholder",
                    "url": "https://www.8percent.kr",
                    "title": "8í¼ì„¼íŠ¸ - AI ê¸°ë°˜ P2P ê¸ˆìœµ",
                    "snippet": "ë¨¸ì‹ ëŸ¬ë‹ ì‹ ìš©í‰ê°€ ëª¨ë¸, ë¦¬ìŠ¤í¬ ê´€ë¦¬ ì‹œìŠ¤í…œ",
                    "content": "8í¼ì„¼íŠ¸ëŠ” AI ê¸°ë°˜ ì‹ ìš©í‰ê°€ ì‹œìŠ¤í…œì„ í†µí•´ P2P ëŒ€ì¶œ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” í•€í…Œí¬ ê¸°ì—…ì…ë‹ˆë‹¤.",
                },
                {
                    "source": "placeholder",
                    "url": "https://www.rainist.com",
                    "title": "ë ˆì´ë‹ˆìŠ¤íŠ¸(ë±…í¬ìƒëŸ¬ë“œ) - ê¸ˆìœµ ë°ì´í„° ë¶„ì„",
                    "snippet": "AI ê¸°ë°˜ ê¸ˆìœµ ë°ì´í„° ë¶„ì„, ë§ì¶¤í˜• ê¸ˆìœµìƒí’ˆ ì¶”ì²œ",
                    "content": "ë ˆì´ë‹ˆìŠ¤íŠ¸ëŠ” ë±…í¬ìƒëŸ¬ë“œë¥¼ ìš´ì˜í•˜ë©° AI ê¸°ìˆ ë¡œ ê¸ˆìœµ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ê°œì¸í™” ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.",
                },
                {
                    "source": "placeholder",
                    "url": "https://www.dunamu.com",
                    "title": "ë‘ë‚˜ë¬´ - AI ê¸ˆìœµ í”Œë«í¼",
                    "snippet": "AI ê¸°ë°˜ ìì‚°ê´€ë¦¬, ê±°ë˜ ì‹œìŠ¤í…œ, ë¦¬ìŠ¤í¬ ê´€ë¦¬",
                    "content": "ë‘ë‚˜ë¬´ëŠ” ì—…ë¹„íŠ¸ë¥¼ ìš´ì˜í•˜ë©° AI ê¸°ìˆ ì„ í™œìš©í•œ ê¸ˆìœµ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” í•€í…Œí¬ ê¸°ì—…ì…ë‹ˆë‹¤.",
                },
            ]
            search_results_raw = docs[:limit]

        docs = _dedupe_by_url(docs)[:limit * 2]

        # LLM Structured Outputìœ¼ë¡œ ì •í™•í•œ ì¶”ì¶œ (from single file)
        candidates: List[Dict[str, Any]] = []
        
        if cfg.use_structured_output and LANGCHAIN_AVAILABLE and PYDANTIC_AVAILABLE:
            try:
                llm = ChatOpenAI(model=cfg.model, temperature=cfg.temperature)
                structured_llm = llm.with_structured_output(StartupSearchResult)
                
                sys_prompt = SYSTEM_PROMPT.format(limit=limit)
                user_prompt = USER_QUERY_TMPL.format(
                    query=query,
                    results=json.dumps(search_results_raw, ensure_ascii=False)
                )
                
                result: StartupSearchResult = structured_llm.invoke([
                    SystemMessage(content=sys_prompt),
                    HumanMessage(content=user_prompt)
                ])
                
                # ì¤‘ë³µ ì œê±°
                uniq = {}
                for item in result.items:
                    key = item.name.strip().lower()
                    if key in uniq:
                        continue
                    uniq[key] = True
                    
                    # URL ì°¾ê¸° (ê²€ìƒ‰ ê²°ê³¼ì—ì„œ)
                    item_url = ""
                    for d in docs:
                        if item.name.lower() in d.get("title", "").lower():
                            item_url = d.get("url", "")
                            break
                    
                    candidates.append({
                        "name": item.name.strip(),
                        "company": item.name.strip(),  # í˜¸í™˜ì„±
                        "domain": item.domain.strip(),
                        "description": item.description.strip(),
                        "url": item_url,
                        "score": 0.95,  # structured outputì€ ë†’ì€ ì‹ ë¢°ë„
                    })
                
                candidates = candidates[:limit]
                
            except Exception as e:
                print(f"âš ï¸ Structured output ì‹¤íŒ¨, ê¸°ë³¸ ë°©ì‹ ì‚¬ìš©: {e}")
                # í´ë°±: ê¸°ì¡´ ë°©ì‹
                candidates = _fallback_candidates_extraction(docs, limit)
        else:
            # ê¸°ì¡´ ë°©ì‹
            candidates = _fallback_candidates_extraction(docs, limit)

        state["startup_search"] = {
            "query": query,
            "candidates": candidates,
            "docs": docs,
            "ts": int(time.time()),
        }
        
        # ì‹¤í–‰ ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*80)
        print("âœ… [1/6] ìŠ¤íƒ€íŠ¸ì—… ê²€ìƒ‰ ì™„ë£Œ")
        print("="*80)
        print(f"ğŸ“Š ë°œê²¬ëœ ìŠ¤íƒ€íŠ¸ì—…: {len(candidates)}ê°œ")
        for idx, c in enumerate(candidates[:3], 1):
            print(f"   {idx}. {c.get('name', 'N/A')} - {c.get('domain', 'N/A')}")
            print(f"      {c.get('description', '')[:60]}...")
        if len(candidates) > 3:
            print(f"   ... ì™¸ {len(candidates)-3}ê°œ")
        print("="*80 + "\n")
        
        return state

    return node

def _fallback_candidates_extraction(docs: List[Dict[str, Any]], limit: int) -> List[Dict[str, Any]]:
    """ê¸°ì¡´ ë°©ì‹ì˜ í›„ë³´ ì¶”ì¶œ (í´ë°±ìš©)"""
    candidates: List[Dict[str, Any]] = []
    for d in docs:
        company = _extract_company_from_title(d.get("title", ""))
        score = _score(d.get("title", ""), d.get("snippet", ""))
        candidates.append({
            "name": company,
            "company": company,
            "title": d.get("title"),
            "url": d.get("url"),
            "snippet": d.get("snippet"),
            "score": round(float(score), 3),
            "domain": "",
            "description": d.get("snippet", "")[:200]
        })
    candidates.sort(key=lambda x: x["score"], reverse=True)
    return candidates[:limit]

# ----------------------------- Graph Builder -----------------------------

def build_startup_search_graph(config: Optional[StartupSearchConfig] = None):
    cfg = config or StartupSearchConfig()
    if not LANGGRAPH_AVAILABLE:
        raise ImportError("LangGraphê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. `pip install langgraph` í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
    g = StateGraph(StartupSearchState)  # ìƒíƒœ íƒ€ì… íŒíŠ¸
    g.add_node("startup_search", _startup_search_node_factory(cfg))
    g.add_edge(START, "startup_search")
    g.add_edge("startup_search", END)
    return g.compile()

# ----------------------------- Helper (Direct Invoke) -----------------------------

def run_startup_search(state: Dict[str, Any], config: Optional[StartupSearchConfig] = None) -> Dict[str, Any]:
    """ê·¸ë˜í”„ë¥¼ ë¹Œë“œí•˜ê³  ë‹¨ì¼ ë…¸ë“œë¥¼ ì‹¤í–‰í•˜ëŠ” í—¬í¼."""
    app = build_startup_search_graph(config)
    return app.invoke(state)


# ----------------------------- Output Formatting -----------------------------

def print_startup_search_results(result: Dict[str, Any]):
    """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥"""
    search = result.get("startup_search", {})
    candidates = search.get("candidates", [])
    
    print("\n" + "=" * 80)
    print("ğŸ” ìŠ¤íƒ€íŠ¸ì—… ê²€ìƒ‰ ê²°ê³¼")
    print("=" * 80)
    print(f"\nê²€ìƒ‰ ì¿¼ë¦¬: {search.get('query', 'N/A')}")
    print(f"ì´ ë°œê²¬: {len(candidates)}ê°œ ìŠ¤íƒ€íŠ¸ì—…")
    print("\n" + "-" * 80)
    
    for idx, candidate in enumerate(candidates, 1):
        print(f"\n[{idx}] {candidate.get('name', candidate.get('company', 'Unknown'))}")
        if candidate.get('domain'):
            print(f"    ğŸ“‚ ë„ë©”ì¸: {candidate['domain']}")
        if candidate.get('description'):
            print(f"    ğŸ’¡ ì„¤ëª…: {candidate['description']}")
        if candidate.get('url'):
            print(f"    ğŸ”— URL: {candidate['url']}")
        print(f"    â­ ì ìˆ˜: {candidate.get('score', 0):.2f}")
    
    print("\n" + "=" * 80)


# ----------------------------- CLI Test -----------------------------
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="AI Fintech Startup Finder (LangGraph)")
    parser.add_argument("--segment", default="fintech ai", help="ê²€ìƒ‰ ì„¸ê·¸ë¨¼íŠ¸")
    parser.add_argument("--region", default="Korea", help="ì§€ì—­ (Korea|Global)")
    parser.add_argument("--limit", type=int, default=10, help="ê²€ìƒ‰í•  ìŠ¤íƒ€íŠ¸ì—… ìˆ˜")
    parser.add_argument("--no-structured", action="store_true", help="Structured output ë¹„í™œì„±í™”")
    args = parser.parse_args()
    
    print("=" * 80)
    print("ğŸ” í•œêµ­ AI í•€í…Œí¬ ìŠ¤íƒ€íŠ¸ì—… ê²€ìƒ‰")
    print("=" * 80)
    print(f"\nì„¤ì •:")
    print(f"  - ì„¸ê·¸ë¨¼íŠ¸: {args.segment}")
    print(f"  - ì§€ì—­: {args.region}")
    print(f"  - ìµœëŒ€ ê°œìˆ˜: {args.limit}")
    print(f"  - Structured Output: {not args.no_structured}")
    
    initial: StartupSearchState = {
        "segment": args.segment,
        "region": args.region,
        "limit": args.limit,
        "language": "ko"
    }
    
    config = StartupSearchConfig(
        use_structured_output=not args.no_structured,
        max_results=args.limit
    )
    
    try:
        final = run_startup_search(initial, config)
        print_startup_search_results(final)
        
        # JSON ì¶œë ¥ (ì˜µì…˜)
        if final.get("startup_search", {}).get("candidates"):
            output_file = "startup_search_results.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(final.get("startup_search"), f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_file}")
            
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
