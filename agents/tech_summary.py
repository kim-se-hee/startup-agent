"""
Module: agents/tech_summary.py (LangGraph + ì›ì²œ ê¸°ìˆ  ë¶„ì„ í†µí•© ë²„ì „)
Purpose: "ê¸°ìˆ  ìš”ì•½" ì—ì´ì „íŠ¸ (ì›ì²œ ê¸°ìˆ  ìƒì„¸ ë¶„ì„, LangGraph ê¸°ë°˜)

íŠ¹ì§•:
- êµ¬ì²´ì ì¸ AI/ML ì•Œê³ ë¦¬ì¦˜, ëª¨ë¸, í”„ë ˆì„ì›Œí¬ ì‹ë³„
- CompanyDetail (íšŒì‚¬ + ì œí’ˆ ë¦¬ìŠ¤íŠ¸) êµ¬ì¡°ë¡œ ì¶œë ¥
- ì›¹ ê²€ìƒ‰ì„ í†µí•œ ê¸°ìˆ  ì •ë³´ ìˆ˜ì§‘
- Pydantic Structured Output ì§€ì›

ì…ë ¥ State:
state = {
  "startup_search": {
      "candidates": [{"name": "í† ìŠ¤", "domain": "Fintech/ê²°ì œ", "description": "..."}],
      "docs": [...]
  },
  "target_company": "í† ìŠ¤"   # (ì„ íƒ)
}

ì¶œë ¥ State:
state["tech_summary"] = {
  "company": "í† ìŠ¤",
  "summary": "...",
  "strengths": [...],
  "weaknesses": [...],
  "tech_score": 0~100,
  "sources": [...],
  # ì‹ ê·œ: CompanyDetail êµ¬ì¡°
  "company_detail": {
      "company": {"name": "", "domain": "", "description": ""},
      "products": [
          {"name": "", "description": "", "strengths": [], "limitations": []}
      ]
  }
}
"""
from __future__ import annotations

import os
import json
from typing import Any, Dict, List, Optional, TypedDict
from dataclasses import dataclass
from dotenv import load_dotenv

# .env ë¡œë“œ
load_dotenv()

# LangGraph
try:
    from langgraph.graph import StateGraph, START, END
    LANGGRAPH_AVAILABLE = True
except Exception:
    LANGGRAPH_AVAILABLE = False

# LangChain
try:
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_core.output_parsers import StrOutputParser
    from langchain_openai import ChatOpenAI
    from langchain_core.messages import SystemMessage, HumanMessage
    LANGCHAIN_CHAIN_AVAILABLE = True
except Exception:
    LANGCHAIN_CHAIN_AVAILABLE = False

# Tavily (ì›¹ ê²€ìƒ‰)
try:
    from tavily import TavilyClient
    TAVILY_AVAILABLE = True
except Exception as e:
    TAVILY_AVAILABLE = False
    print(f"âš ï¸ Tavily import ì‹¤íŒ¨: {e}")

# Pydantic (ìŠ¤í‚¤ë§ˆ)
try:
    from pydantic import BaseModel, Field, ConfigDict
    
    class Company(BaseModel):
        """íšŒì‚¬ ê¸°ë³¸ ì •ë³´"""
        name: str
        domain: str
        desription: str  # ì˜¤íƒ€ ìœ ì§€ (market.py í˜¸í™˜ì„±)
        model_config = ConfigDict(str_strip_whitespace=True)
    
    class Product(BaseModel):
        """ì œí’ˆ/ì„œë¹„ìŠ¤ ì •ë³´"""
        name: str
        description: str
        strengths: List[str] = []
        limitations: List[str] = []
        model_config = ConfigDict(str_strip_whitespace=True)
    
    class CompanyDetail(BaseModel):
        """íšŒì‚¬ ìƒì„¸ ì •ë³´ (íšŒì‚¬ + ì œí’ˆë“¤)"""
        company: Company
        products: List[Product]
        model_config = ConfigDict(str_strip_whitespace=True)
    
    PYDANTIC_AVAILABLE = True
except Exception:
    PYDANTIC_AVAILABLE = False
    Company = dict
    Product = dict
    CompanyDetail = dict


# ----------------------------- State & Config -----------------------------
class TechSummaryOutput(TypedDict, total=False):
    company: str
    summary: str
    strengths: List[str]
    weaknesses: List[str]
    tech_score: float
    sources: List[Dict[str, str]]
    # ì‹ ê·œ: CompanyDetail êµ¬ì¡°
    company_detail: Dict[str, Any]
    products: List[Dict[str, Any]]

class TechSummaryState(TypedDict, total=False):
    startup_search: Dict[str, Any]
    target_company: str
    tech_summary: TechSummaryOutput

@dataclass
class TechSummaryConfig:
    model: str = "gpt-4o-mini"
    temperature: float = 0.0  # ì›ì²œ ê¸°ìˆ  ë¶„ì„ì€ ì •í™•ì„± ì¤‘ìš”
    include_chars: int = 1800
    use_structured_output: bool = True  # CompanyDetail structured output ì‚¬ìš©
    collect_web_info: bool = True  # ì¶”ê°€ ì›¹ ê²€ìƒ‰ ì—¬ë¶€


# ----------------------------- Helpers -----------------------------
def _norm(s: str) -> str:
    return (s or "").strip()

def _normalize_company_name(name: str) -> str:
    """íšŒì‚¬ ì´ë¦„ ì •ê·œí™” (ê´„í˜¸ ì œê±° ë“±)"""
    import re
    # ê´„í˜¸ì™€ ë‚´ìš© ì œê±°: "PFCT(í”¼ì—í”„ì”¨í…Œí¬ë†€ë¡œì§€ìŠ¤)" -> "PFCT"
    name = re.sub(r'\([^)]*\)', '', name).strip()
    # ì—°ì† ê³µë°± ì œê±°
    name = re.sub(r'\s+', ' ', name).strip()
    return name

def _print_tech_summary_result(tech_summary: Dict[str, Any]):
    """ê¸°ìˆ  ìš”ì•½ ê²°ê³¼ ì¶œë ¥"""
    print("\n" + "="*80)
    print("âœ… [2/6] ê¸°ìˆ  ìš”ì•½ ì™„ë£Œ")
    print("="*80)
    print(f"ğŸ¢ íšŒì‚¬: {tech_summary.get('company', 'N/A')}")
    print(f"ğŸ“Š ê¸°ìˆ  ì ìˆ˜: {tech_summary.get('tech_score', 0):.1f}/100")
    
    strengths = tech_summary.get('strengths', [])
    if strengths:
        print(f"ğŸ’ª ê¸°ìˆ  ê°•ì :")
        for idx, s in enumerate(strengths[:3], 1):
            print(f"   {idx}. {s}")
    
    company_detail = tech_summary.get('company_detail', {})
    products = company_detail.get('products', [])
    if products:
        print(f"ğŸ“¦ ì œí’ˆ/ì„œë¹„ìŠ¤: {len(products)}ê°œ")
        for idx, p in enumerate(products[:2], 1):
            print(f"   {idx}. {p.get('name', 'N/A')}: {p.get('description', '')[:50]}...")
    
    print("="*80 + "\n")

def _choose_company(state: TechSummaryState) -> tuple:
    """íšŒì‚¬ ì •ë³´ ì„ íƒ (name, domain, description)"""
    if _norm(state.get("target_company", "")):
        company_name = _norm(state["target_company"])
        # startup_searchì—ì„œ ì¶”ê°€ ì •ë³´ ì°¾ê¸°
        cands = (state.get("startup_search", {}) or {}).get("candidates", [])
        for c in cands:
            if _norm(c.get("name", "")).lower() == company_name.lower():
                return (c.get("name"), c.get("domain", ""), c.get("description", ""))
        return (company_name, "", "")
    
    cands = (state.get("startup_search", {}) or {}).get("candidates", [])
    if not cands:
        return ("(unknown)", "", "")
    
    c = cands[0]
    return (
        _norm(c.get("name") or c.get("company") or c.get("title", "").split(" - ")[0]),
        _norm(c.get("domain", "")),
        _norm(c.get("description", ""))
    )

def _related_docs(state: TechSummaryState, company: str, max_docs: int = 8, max_chars: int = 2000) -> List[Dict[str, str]]:
    """ê¸°ì¡´ ë¬¸ì„œì—ì„œ ê´€ë ¨ ë¬¸ì„œ í•„í„°ë§"""
    docs = (state.get("startup_search", {}) or {}).get("docs", [])
    company_l = company.lower()
    picked: List[Dict[str, str]] = []
    for d in docs:
        title = _norm(d.get("title", ""))
        url = _norm(d.get("url", ""))
        content = _norm(d.get("content", "")) or _norm(d.get("snippet", ""))
        if company_l in title.lower() or company_l in url.lower() or company_l in content.lower():
            picked.append({"title": title[:240], "url": url, "content": content[:max_chars]})
    if not picked:
        for d in docs[:3]:
            picked.append({
                "title": _norm(d.get("title", ""))[:240],
                "url": _norm(d.get("url", "")),
                "content": (_norm(d.get("content", "")) or _norm(d.get("snippet", "")))[:max_chars]
            })
    return picked[:max_docs]

def collect_company_tech_info(company_name: str, company_domain: str) -> str:
    """íšŒì‚¬ì— ëŒ€í•œ ê¸°ìˆ  ì •ë³´ë¥¼ ì›¹ì—ì„œ ìˆ˜ì§‘ (from single file)"""
    if not TAVILY_AVAILABLE:
        return f"{company_name}ì— ëŒ€í•œ ê¸°ìˆ  ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    # íšŒì‚¬ ì´ë¦„ ì •ê·œí™” (ê´„í˜¸ ì œê±°)
    normalized_name = _normalize_company_name(company_name)
    print(f"ğŸ” ê¸°ìˆ  ì •ë³´ ê²€ìƒ‰: {normalized_name}")
    
    collected_info = []
    
    search_queries = [
        f"{normalized_name} ê¸°ìˆ  ìŠ¤íƒ architecture AI",
        f"{normalized_name} AI ëª¨ë¸ ì•Œê³ ë¦¬ì¦˜ ë¨¸ì‹ ëŸ¬ë‹",
        f"{normalized_name} ê°œë°œ ê¸°ìˆ  ë¸”ë¡œê·¸ tech",
        f"{normalized_name} engineering blog",
        f"{normalized_name} íŠ¹í—ˆ ë…¼ë¬¸ patent"
    ]
    
    tavily_key = os.getenv("TAVILY_API_KEY")
    if not tavily_key:
        return f"{company_name}ì— ëŒ€í•œ ê¸°ìˆ  ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (Tavily API Key ì—†ìŒ)"
    
    try:
        tavily_client = TavilyClient(api_key=tavily_key)
    except Exception as e:
        return f"{company_name}ì— ëŒ€í•œ ê¸°ìˆ  ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (Tavily ì´ˆê¸°í™” ì‹¤íŒ¨: {e})"
    
    seen_urls = set()
    collected_count = 0
    
    for query in search_queries[:4]:
        try:
            response = tavily_client.search(query=query, max_results=6)
            results = response.get('results', [])
            
            if not isinstance(results, list):
                continue
            
            for result in results[:6]:
                if not isinstance(result, dict):
                    continue
                    
                title = result.get('title', '').lower()
                content = result.get('content', '')
                url = result.get('url', '')
                
                if url in seen_urls or not content or len(content) < 100:
                    continue
                
                tech_keywords = [
                    'ai', 'ml', 'machine learning', 'deep learning', 'nlp', 
                    'algorithm', 'model', 'ì•Œê³ ë¦¬ì¦˜', 'ëª¨ë¸', 'ë¨¸ì‹ ëŸ¬ë‹', 
                    'ë”¥ëŸ¬ë‹', 'ì¸ê³µì§€ëŠ¥', 'tensorflow', 'pytorch', 'bert', 
                    'gpt', 'transformer', 'lstm', 'xgboost', 'ê¸°ìˆ ', 'tech'
                ]
                
                has_company = normalized_name.lower() in title or normalized_name.lower() in content.lower()
                has_tech = any(keyword in title or keyword in content.lower() for keyword in tech_keywords)
                
                if has_company and has_tech:
                    seen_urls.add(url)
                    collected_info.append(
                        f"[ì¶œì²˜: {result.get('title', 'Unknown')}]\n{content}\n"
                    )
                    collected_count += 1
                    
                    if collected_count >= 6:
                        break
            
            if collected_count >= 6:
                break
                
        except Exception:
            continue
    
    if not collected_info:
        return f"{company_name}ì— ëŒ€í•œ ê¸°ìˆ  ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    return "\n\n".join(collected_info)

def _offline_summarize() -> Dict[str, Any]:
    """ì˜¤í”„ë¼ì¸ ê¸°ë³¸ê°’"""
    return {
        "summary": (
            "ì§€ë„í•™ìŠµê³¼ ì´ìƒíƒì§€ë¥¼ ê²°í•©í•œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë©°, ê±°ë˜ ë¡œê·¸/ë””ë°”ì´ìŠ¤ ì‹ í˜¸ ë°ì´í„°ë¥¼ í™œìš©. "
            "ì‹¤ì‹œê°„ ì¶”ë¡  íŒŒì´í”„ë¼ì¸ê³¼ ëª¨ë‹ˆí„°ë§ ì²´ê³„ë¥¼ ë³´ìœ í•˜ê³ , API í˜•íƒœë¡œ ê¸ˆìœµê¸°ê´€ ì›Œí¬í”Œë¡œìš°ì— í†µí•© ê°€ëŠ¥."
        ),
        "strengths": ["ì‹¤ì‹œê°„ ì²˜ë¦¬ ì•„í‚¤í…ì²˜", "ë°ì´í„° í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ìì‚°", "í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ ìš´ì˜"],
        "weaknesses": ["ì„¤ëª…ê°€ëŠ¥ì„±/ê·œì œ ì í•©ì„± ê³¼ì œ", "ë°ì´í„° í¸í–¥ ë¦¬ìŠ¤í¬"],
        "tech_score": 82,
    }


# ----------------------------- Prompts (from single file) -----------------------------

TECH_SUMMARY_SYSTEM_PROMPT = """ë‹¹ì‹ ì€ í•€í…Œí¬ ì›ì²œ ê¸°ìˆ  ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**ë¶„ì„ ëª©í‘œ: êµ¬ì²´ì ì¸ AI ì›ì²œ ê¸°ìˆ  íŒŒì•…**

í•µì‹¬ ì›ì¹™:
1. **êµ¬ì²´ì ì¸ ê¸°ìˆ ë§Œ ì¶”ì¶œ**: "AI í™œìš©" ê°™ì€ ì¼ë°˜ì  í‘œí˜„ ê¸ˆì§€
2. **ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜**: ëª…ì‹œëœ ë‚´ìš©ë§Œ ì‚¬ìš©, ì¶”ì¸¡ ê¸ˆì§€
3. **ê¸°ìˆ  ìŠ¤íƒ ìš°ì„ **: ì•Œê³ ë¦¬ì¦˜, ëª¨ë¸, í”„ë ˆì„ì›Œí¬ ë“± êµ¬ì²´ì  ê¸°ìˆ  ì •ë³´
4. **ê°•ì /í•œê³„ í•„ìˆ˜**: ê° ì œí’ˆì˜ strengthsì™€ limitationsë¥¼ ë°˜ë“œì‹œ ë¶„ì„í•˜ì—¬ ì¶”ì¶œ

ë¶„ì„ ì‘ì—…:

1. **ì œí’ˆ/ì„œë¹„ìŠ¤ ì‹ë³„**
   - ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ëª…í™•í•œ ì œí’ˆ/ì„œë¹„ìŠ¤ë§Œ ì¶”ì¶œ
   - ì—¬ëŸ¬ ì œí’ˆì´ ìˆìœ¼ë©´ ê°ê° ë³„ë„ ê°ì²´ë¡œ ìƒì„±

2. **ì›ì²œ ê¸°ìˆ  ì„¤ëª… (description) - ê°€ì¥ ì¤‘ìš”**
   ë‹¤ìŒ ì •ë³´ë¥¼ ìš°ì„  ìˆœìœ„ëŒ€ë¡œ ì¶”ì¶œ:
   
   a) **AI/ML ì•Œê³ ë¦¬ì¦˜**: 
      - ì˜ˆ: "Random Forest ê¸°ë°˜ ì‹ ìš©í‰ê°€", "LSTM ì‹œê³„ì—´ ì˜ˆì¸¡", "Transformer ê¸°ë°˜ NLP"
      - ì˜ˆ: "XGBoost ë¦¬ìŠ¤í¬ ëª¨ë¸", "ê°•í™”í•™ìŠµ ì¶”ì²œ ì‹œìŠ¤í…œ"
      
   b) **êµ¬ì²´ì  ëª¨ë¸**:
      - ì˜ˆ: "BERT íŒŒì¸íŠœë‹", "GPT ê¸°ë°˜ ì±—ë´‡", "CNN ë¬¸ì„œ ë¶„ì„"
      
   c) **ê¸°ìˆ  í”„ë ˆì„ì›Œí¬/ë¼ì´ë¸ŒëŸ¬ë¦¬**:
      - ì˜ˆ: "TensorFlow", "PyTorch", "scikit-learn", "Keras"
      
   d) **ë°ì´í„° ì²˜ë¦¬ ê¸°ìˆ **:
      - ì˜ˆ: "ë¹„ì •í˜• ë°ì´í„° ì²˜ë¦¬", "ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬", "ëŒ€ê·œëª¨ ë¶„ì‚° ì²˜ë¦¬"

3. **ê¸°ìˆ ì  ê°•ì  (strengths) - í•„ìˆ˜**
   ì œí’ˆ/ê¸°ìˆ ì˜ ì¥ì ì„ ì¶”ì¶œ:
   - ì˜ˆ: "ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°€ëŠ¥", "ë†’ì€ ì •í™•ë„", "ë‚®ì€ ìš´ì˜ ë¹„ìš©"
   - ì˜ˆ: "ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬", "ë¹ ë¥¸ ì‘ë‹µ ì†ë„", "ìë™í™”ëœ ì˜ì‚¬ê²°ì •"
   - ê²€ìƒ‰ ê²°ê³¼ì— ëª…ì‹œëœ ë‚´ìš©ë§Œ ì‚¬ìš©
   - ìµœì†Œ 2-3ê°œ ì´ìƒ ì¶”ì¶œ ì‹œë„

4. **ê¸°ìˆ ì  í•œê³„ (limitations) - í•„ìˆ˜**
   ì œí’ˆ/ê¸°ìˆ ì˜ ì œì•½ì‚¬í•­ ì¶”ì¶œ:
   - ì˜ˆ: "ì„¤ëª…ê°€ëŠ¥ì„± ë¶€ì¡±", "ë°ì´í„° í¸í–¥ ê°€ëŠ¥ì„±", "ì´ˆê¸° êµ¬ì¶• ë¹„ìš©"
   - ì˜ˆ: "íŠ¹ì • ë„ë©”ì¸ ì œí•œ", "í•™ìŠµ ë°ì´í„° ì˜ì¡´ì„±", "ì •ê¸°ì  ì¬í•™ìŠµ í•„ìš”"
   - ê²€ìƒ‰ ê²°ê³¼ì— ëª…ì‹œëœ ë‚´ìš©ë§Œ ì‚¬ìš©
   - ìµœì†Œ 1-2ê°œ ì´ìƒ ì¶”ì¶œ ì‹œë„
      
   e) **íŠ¹í™” ê¸°ìˆ **:
      - ì˜ˆ: "ìì—°ì–´ì²˜ë¦¬(NLP)", "ì»´í“¨í„° ë¹„ì „(CV)", "ì¶”ì²œ ì—”ì§„", "ì´ìƒ íƒì§€"

   **ì‘ì„± í˜•ì‹**:
   "[ì„œë¹„ìŠ¤ëª…]: [êµ¬ì²´ì  AI ê¸°ìˆ ] ê¸°ë°˜ì˜ [ê¸ˆìœµ ì„œë¹„ìŠ¤]. [ì¶”ê°€ ê¸°ìˆ  ìƒì„¸]"
   
   **ì¢‹ì€ ì˜ˆì‹œ**:
   - "AI ì‹ ìš©í‰ê°€: Gradient Boosting(XGBoost) ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜ ëŒ€ì•ˆì‹ ìš©í‰ê°€ ëª¨ë¸. ë¹„ì •í˜• ë°ì´í„°(í†µì‹ , ê²°ì œ ë“±) í•™ìŠµ"
   - "ì±—ë´‡: BERT íŒŒì¸íŠœë‹í•œ ê¸ˆìœµ ìƒë‹´ NLP ëª¨ë¸. ì˜ë„ ë¶„ë¥˜ ë° ì—”í‹°í‹° ì¶”ì¶œ"
   
   **ë‚˜ìœ ì˜ˆì‹œ**:
   - "AI ê¸°ìˆ ì„ í™œìš©í•œ ì‹ ìš©í‰ê°€" âŒ
   - "ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì„œë¹„ìŠ¤" âŒ

   **ì •ë³´ ë¶€ì¡±ì‹œ**: "í•€í…Œí¬ [ë¶„ì•¼] ì„œë¹„ìŠ¤ (êµ¬ì²´ì  ê¸°ìˆ  ì •ë³´ ë¶€ì¡±)"

3. **ì¥ì  (strengths)**
   - ê¸°ìˆ ì  ì°¨ë³„ì , ì„±ê³¼, ì •í™•ë„ ê°œì„  ë“±
   - êµ¬ì²´ì  ìˆ˜ì¹˜ í¬í•¨ (ì˜ˆ: "ì •í™•ë„ 15% í–¥ìƒ")
   - ìµœì†Œ 2ê°œ, ìµœëŒ€ 5ê°œ
   - **ì •ë³´ ì—†ìœ¼ë©´: ë¹ˆ ë¦¬ìŠ¤íŠ¸ []**

4. **í•œê³„ì  (limitations)**
   - ê¸°ìˆ ì  ì œì•½, ê°œì„  ê³¼ì œ
   - **ì •ë³´ ì—†ìœ¼ë©´: ë¹ˆ ë¦¬ìŠ¤íŠ¸ []**

ì¶œë ¥ ìŠ¤í‚¤ë§ˆ (JSONë§Œ):
{{
  "company": {{"name": "", "domain": "", "desription": ""}},
  "products": [
    {{
      "name": "",
      "description": "",
      "strengths": [],
      "limitations": []
    }}
  ]
}}
"""

TECH_SUMMARY_USER_QUERY_TMPL = """ë‹¤ìŒ íšŒì‚¬ì˜ ì›ì²œ ê¸°ìˆ  ì •ë³´ë¥¼ ë¶„ì„í•˜ì„¸ìš”.

íšŒì‚¬ ì •ë³´:
- ì´ë¦„: {company_name}
- ë¶„ì•¼: {company_domain}  
- ê°œìš”: {company_desription}

=== ìˆ˜ì§‘ëœ ê¸°ìˆ  ì •ë³´ ===
{search_results}
========================

**ë¶„ì„ ì§€ì¹¨:**
1. êµ¬ì²´ì ì¸ AI/ML ì•Œê³ ë¦¬ì¦˜, ëª¨ë¸, í”„ë ˆì„ì›Œí¬ë¥¼ ì°¾ìœ¼ì„¸ìš”
2. "AI í™œìš©", "ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜" ê°™ì€ ì¼ë°˜ì  í‘œí˜„ì€ í”¼í•˜ì„¸ìš”
3. ê° ì œí’ˆë§ˆë‹¤ **strengths(ê°•ì )ì™€ limitations(í•œê³„)**ë¥¼ ë°˜ë“œì‹œ ì¶”ì¶œí•˜ì„¸ìš”
   - ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ê¸°ìˆ ì˜ ì¥ì , íŠ¹ì§•, ì´ì  â†’ strengths
   - ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì œì•½ì‚¬í•­, í•œê³„, ë¦¬ìŠ¤í¬ â†’ limitations
   - ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ ì¼ë°˜ì ì¸ AI ê¸°ìˆ ì˜ ê°•ì /í•œê³„ ì¶”ë¡  ê°€ëŠ¥
4. ê²€ìƒ‰ ê²°ê³¼ì— ëª…ì‹œëœ ë‚´ìš©ì„ ìš°ì„ í•˜ë˜, ì œí’ˆ íŠ¹ì„±ìƒ ëª…í™•í•œ ê°•ì /í•œê³„ëŠ” ì¶”ë¡  ê°€ëŠ¥

**í•„ìˆ˜ ì¶œë ¥:**
- ê° ì œí’ˆì˜ strengths: ìµœì†Œ 2-3ê°œ
- ê° ì œí’ˆì˜ limitations: ìµœì†Œ 1-2ê°œ

JSONìœ¼ë¡œë§Œ ë°˜í™˜í•˜ì„¸ìš”.
"""


# ----------------------------- Node Factory -----------------------------
def _get_llm(model: str, temperature: float):
    if not LANGCHAIN_CHAIN_AVAILABLE or not os.getenv("OPENAI_API_KEY"):
        return None
    try:
        return ChatOpenAI(model=model, temperature=temperature)
    except Exception:
        return None

def _llm_json(prompt: str, llm) -> Dict[str, Any]:
    if not llm:
        return _offline_summarize()
    try:
        chain = ChatPromptTemplate.from_template("{prompt}") | llm | StrOutputParser()
        txt = chain.invoke({"prompt": prompt})
        return json.loads(txt)
    except Exception:
        return _offline_summarize()

def _tech_summary_node_factory(cfg: TechSummaryConfig):
    def node(state: TechSummaryState) -> TechSummaryState:
        company_name, company_domain, company_desc = _choose_company(state)
        
        # Structured Output ë°©ì‹ (from single file)
        if cfg.use_structured_output and LANGCHAIN_CHAIN_AVAILABLE and PYDANTIC_AVAILABLE:
            try:
                # ì›¹ì—ì„œ ê¸°ìˆ  ì •ë³´ ì¶”ê°€ ìˆ˜ì§‘
                if cfg.collect_web_info:
                    search_results = collect_company_tech_info(company_name, company_domain)
                else:
                    # ê¸°ì¡´ ë¬¸ì„œë§Œ ì‚¬ìš©
                    docs = _related_docs(state, company_name, max_docs=8, max_chars=cfg.include_chars)
                    search_results = "\n\n".join([f"[{d['title']}]\n{d['content']}" for d in docs])
                
                if "ê¸°ìˆ  ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in search_results:
                    # ê¸°ë³¸ê°’ ë°˜í™˜
                    state["tech_summary"] = {
                        "company": company_name,
                        "summary": f"{company_name}ì˜ ê¸°ìˆ  ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                        "strengths": [],
                        "weaknesses": [],
                        "tech_score": 50,
                        "sources": [],
                        "company_detail": {
                            "company": {"name": company_name, "domain": company_domain, "desription": company_desc},  # ì˜¤íƒ€ í•„ë“œëª…
                            "products": []
                        }
                    }
                    _print_tech_summary_result(state["tech_summary"])
                    return state
                
                # LLM Structured Output
                llm = ChatOpenAI(model=cfg.model, temperature=cfg.temperature)
                structured_llm = llm.with_structured_output(CompanyDetail)
                
                sys_prompt = TECH_SUMMARY_SYSTEM_PROMPT
                user_prompt = TECH_SUMMARY_USER_QUERY_TMPL.format(
                    company_name=company_name,
                    company_domain=company_domain,
                    company_desription=company_desc,  # ì˜¤íƒ€ í•„ë“œëª…
                    search_results=search_results
                )
                
                detail: CompanyDetail = structured_llm.invoke([
                    SystemMessage(content=sys_prompt),
                    HumanMessage(content=user_prompt)
                ])
                
                # CompanyDetailì„ ë ˆê±°ì‹œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                all_strengths = []
                all_limitations = []
                product_summaries = []
                
                for prod in detail.products:
                    product_summaries.append(f"{prod.name}: {prod.description}")
                    all_strengths.extend(prod.strengths)
                    all_limitations.extend(prod.limitations)
                
                summary_text = "\n".join(product_summaries) if product_summaries else company_desc
                
                # ê¸°ìˆ  ì ìˆ˜ ê³„ì‚° (ì œí’ˆ ìˆ˜ì™€ ê¸°ìˆ  ìƒì„¸ë„ ê¸°ë°˜)
                tech_score = min(100, 60 + len(detail.products) * 10 + len(all_strengths) * 3)
                
                state["tech_summary"] = {
                    "company": company_name,
                    "summary": summary_text,
                    "strengths": all_strengths[:5],
                    "weaknesses": all_limitations[:5],
                    "tech_score": float(tech_score),
                    "sources": [],
                    # CompanyDetail ì¶”ê°€ (market_evalì—ì„œ ì‚¬ìš©)
                    "company_detail": {
                        "company": {
                            "name": detail.company.name,
                            "desription": detail.company.desription,  # ì˜¤íƒ€ í•„ë“œëª… (í˜¸í™˜ì„±)
                            "domain": detail.company.domain
                        },
                        "products": [
                            {
                                "name": p.name,
                                "description": p.description,
                                "strengths": p.strengths,
                                "limitations": p.limitations
                            } for p in detail.products
                        ]
                    },
                    "products": [p.model_dump() if hasattr(p, 'model_dump') else p for p in detail.products]
                }
                _print_tech_summary_result(state["tech_summary"])
                return state
                
            except Exception as e:
                print(f"âš ï¸ Structured output ì‹¤íŒ¨, ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì „í™˜: {e}")
        
        # ê¸°ì¡´ ë°©ì‹ (í´ë°±)
        docs = _related_docs(state, company_name, max_docs=8, max_chars=cfg.include_chars)
        llm = _get_llm(cfg.model, cfg.temperature)

        prompt = f"""
ë„ˆëŠ” í•€í…Œí¬ AI ê¸°ìˆ  ë¶„ì„ê°€ë‹¤. ì•„ë˜ JSON ë¬¸ì„œë¥¼ ê·¼ê±°ë¡œ **{company_name}** ì˜ ê¸°ìˆ ì„ ìš”ì•½í•˜ë¼.
ê·¼ê±° ì—†ëŠ” ì¶”ì •ì€ ê¸ˆì§€í•˜ë©°, ë¶ˆí™•ì‹¤í•˜ë©´ ì–¸ê¸‰í•˜ì§€ ë§ë¼.

ì…ë ¥ ë¬¸ì„œ(JSON): {json.dumps(docs, ensure_ascii=False)[:7000]}

ì•„ë˜ í˜•ì‹ì˜ JSONë§Œ ë°˜í™˜:
{{
  "summary": "í•µì‹¬ ê¸°ìˆ /ì•„í‚¤í…ì²˜/ë°ì´í„°/ì„±ëŠ¥ ìš”ì•½ (8~12ë¬¸ì¥)",
  "strengths": ["ìµœëŒ€ 5ê°œ"],
  "weaknesses": ["ìµœëŒ€ 5ê°œ"],
  "tech_score": 0-100
}}
"""
        data = _llm_json(prompt, llm)
        sources = [{"title": d.get("title", ""), "url": d.get("url", "")} for d in docs[:5]]

        state["tech_summary"] = {
            "company": company_name,
            "summary": data.get("summary", ""),
            "strengths": data.get("strengths", []),
            "weaknesses": data.get("weaknesses", []),
            "tech_score": float(data.get("tech_score", 0)),
            "sources": sources,
        }
        _print_tech_summary_result(state["tech_summary"])
        return state
    return node


# ----------------------------- Graph Builder -----------------------------
def build_tech_summary_graph(config: Optional[TechSummaryConfig] = None):
    cfg = config or TechSummaryConfig()
    if not LANGGRAPH_AVAILABLE:
        raise ImportError("LangGraphê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. `pip install langgraph` í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
    g = StateGraph(TechSummaryState)
    g.add_node("tech_summary", _tech_summary_node_factory(cfg))
    g.add_edge(START, "tech_summary")
    g.add_edge("tech_summary", END)
    return g.compile()


# ----------------------------- Helper for Direct Run -----------------------------
def run_tech_summary(state: Dict[str, Any], config: Optional[TechSummaryConfig] = None) -> Dict[str, Any]:
    app = build_tech_summary_graph(config)
    return app.invoke(state)


# ----------------------------- Output Formatting -----------------------------

def print_tech_summary_results(result: Dict[str, Any]):
    """ê¸°ìˆ  ìš”ì•½ ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥"""
    tech = result.get("tech_summary", {})
    
    print("\n" + "=" * 80)
    print("ğŸ”¬ ê¸°ìˆ  ìš”ì•½ ê²°ê³¼")
    print("=" * 80)
    
    print(f"\nğŸ¢ íšŒì‚¬: {tech.get('company', 'N/A')}")
    print(f"ğŸ“Š ê¸°ìˆ  ì ìˆ˜: {tech.get('tech_score', 0):.1f} / 100")
    
    if tech.get('summary'):
        print(f"\nğŸ’¡ ê¸°ìˆ  ìš”ì•½:")
        print(f"{tech['summary']}")
    
    if tech.get('strengths'):
        print(f"\nâœ… ê°•ì  ({len(tech['strengths'])}ê°œ):")
        for idx, strength in enumerate(tech['strengths'], 1):
            print(f"   {idx}. {strength}")
    
    if tech.get('weaknesses'):
        weakness_count = len(tech.get('weaknesses', []))
        print(f"\nâš ï¸  ì•½ì  ({weakness_count}ê°œ):")
        for idx, weakness in enumerate(tech['weaknesses'], 1):
            print(f"   {idx}. {weakness}")
    
    # CompanyDetail êµ¬ì¡° ì¶œë ¥
    if tech.get('company_detail'):
        detail = tech['company_detail']
        products = detail.get('products', [])
        if products:
            print(f"\nğŸ“¦ ì œí’ˆ/ì„œë¹„ìŠ¤ ({len(products)}ê°œ):")
            for idx, prod in enumerate(products, 1):
                print(f"\n   [{idx}] {prod.get('name', 'N/A')}")
                print(f"       ì„¤ëª…: {prod.get('description', '')[:100]}...")
                if prod.get('strengths'):
                    print(f"       ê°•ì : {', '.join(prod['strengths'][:3])}")
                if prod.get('limitations'):
                    print(f"       í•œê³„: {', '.join(prod['limitations'][:3])}")
    
    print("\n" + "=" * 80)


# ----------------------------- CLI Test -----------------------------
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Tech Summary Agent (LangGraph)")
    parser.add_argument("--company", default="í† ìŠ¤", help="ë¶„ì„í•  íšŒì‚¬ëª…")
    parser.add_argument("--domain", default="Fintech/ê²°ì œ", help="íšŒì‚¬ ë„ë©”ì¸")
    parser.add_argument("--no-web-search", action="store_true", help="ì›¹ ê²€ìƒ‰ ë¹„í™œì„±í™”")
    parser.add_argument("--no-structured", action="store_true", help="Structured output ë¹„í™œì„±í™”")
    args = parser.parse_args()
    
    print("=" * 80)
    print("ğŸ”¬ ê¸°ìˆ  ìš”ì•½ ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    print(f"\nì„¤ì •:")
    print(f"  - íšŒì‚¬: {args.company}")
    print(f"  - ë„ë©”ì¸: {args.domain}")
    print(f"  - ì›¹ ê²€ìƒ‰: {not args.no_web_search}")
    print(f"  - Structured Output: {not args.no_structured}")
    
    # ëª¨ì˜ startup_search ê²°ê³¼
    dummy = {
        "startup_search": {
            "candidates": [{
                "name": args.company,
                "domain": args.domain,
                "description": f"{args.company}ëŠ” AI ê¸°ìˆ ì„ í™œìš©í•˜ëŠ” í•€í…Œí¬ ê¸°ì—…ì…ë‹ˆë‹¤."
            }],
            "docs": []
        },
        "target_company": args.company,
    }
    
    config = TechSummaryConfig(
        use_structured_output=not args.no_structured,
        collect_web_info=not args.no_web_search
    )
    
    try:
        final = run_tech_summary(dummy, config)
        print_tech_summary_results(final)
        
        # JSON ì €ì¥
        output_file = "tech_summary_result.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(final.get("tech_summary"), f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_file}")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()