"""
AI í•€í…Œí¬ ê¸°ìˆ  ìš”ì•½ ì—ì´ì „íŠ¸ (LangGraph State ê¸°ë°˜)
- ìŠ¤íƒ€íŠ¸ì—… ê²€ìƒ‰ â†’ ê¸°ìˆ  ìš”ì•½ì„ LangGraphë¡œ ì—°ê²°
- Stateë¥¼ í†µí•´ ì—ì´ì „íŠ¸ ê°„ ë°ì´í„° ì „ë‹¬
- ë‹¤ìŒ ì—ì´ì „íŠ¸ê°€ ì¶”ê°€ë˜ë©´ company_detailsë¥¼ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥

ì‚¬ìš©ë²•: python ai_tech_summary_single.py --limit 5
"""

import os
import argparse
import json
from typing import List, Dict, Any
from dotenv import load_dotenv

from pydantic import BaseModel, ConfigDict
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_tavily import TavilySearch
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver


# ============================================================================
# í™˜ê²½ ì„¤ì •
# ============================================================================
load_dotenv()

OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY", "")

def ensure_required_env():
    """í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜ í™•ì¸"""
    required = ["OPENAI_API_KEY", "TAVILY_API_KEY"]
    missing = [k for k in required if not os.getenv(k)]
    if missing:
        lines = [f"- {k} ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤." for k in missing]
        msg = "ë‹¤ìŒ í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤:\n" + "\n".join(lines) + "\n(.envì— í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”)"
        raise RuntimeError(msg)

ensure_required_env()


# ============================================================================
# ë°ì´í„° ëª¨ë¸
# ============================================================================
class StartupHit(BaseModel):
    """ìŠ¤íƒ€íŠ¸ì—… ê²€ìƒ‰ ê²°ê³¼"""
    name: str
    domain: str
    description: str

    model_config = ConfigDict(str_strip_whitespace=True)


class StartupSearchResult(BaseModel):
    """LLM êµ¬ì¡°í™” ì¶œë ¥ìš©"""
    items: List[StartupHit] = []


class Company(BaseModel):
    """íšŒì‚¬ ê¸°ë³¸ ì •ë³´"""
    name: str
    domain: str
    description: str

    model_config = ConfigDict(str_strip_whitespace=True)


class Product(BaseModel):
    """ì œí’ˆ/ì„œë¹„ìŠ¤ ì •ë³´"""
    name: str
    description: str
    strengths: List[str]
    limitations: List[str]

    model_config = ConfigDict(str_strip_whitespace=True)


class CompanyDetail(BaseModel):
    """íšŒì‚¬ ìƒì„¸ ì •ë³´ (íšŒì‚¬ + ì œí’ˆë“¤)"""
    company: Company
    products: List[Product]

    model_config = ConfigDict(str_strip_whitespace=True)


# ============================================================================
# LangGraph State ì •ì˜
# ============================================================================
class AgentState(BaseModel):
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ ìƒíƒœ"""
    # ì…ë ¥ íŒŒë¼ë¯¸í„°
    region: str = "Korea"
    limit: int = 5
    
    # 1ë‹¨ê³„: ìŠ¤íƒ€íŠ¸ì—… ê²€ìƒ‰ ê²°ê³¼
    startups: List[StartupHit] = []
    
    # 2ë‹¨ê³„: ê¸°ìˆ  ìš”ì•½ ê²°ê³¼ (ë‹¤ìŒ ì—ì´ì „íŠ¸ë¡œ ì „ë‹¬ë  ë°ì´í„°)
    company_details: List[CompanyDetail] = []
    
    # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
    current_step: str = "init"
    error: str = ""
    
    model_config = ConfigDict(arbitrary_types_allowed=True)


# ============================================================================
# ë„êµ¬ (Tools)
# ============================================================================
_tavily = TavilySearch(
    max_results=30,
    api_key=TAVILY_API_KEY
)

def web_search(query: str) -> List[Dict[str, Any]]:
    """ì›¹ ê²€ìƒ‰"""
    try:
        result = _tavily.invoke({"query": query})
        if isinstance(result, dict):
            return result.get('results', [])
        elif isinstance(result, list):
            return result
        else:
            return []
    except Exception as e:
        print(f"   âš ï¸  ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []


# ============================================================================
# í”„ë¡¬í”„íŠ¸
# ============================================================================
STARTUP_SEARCH_SYSTEM_PROMPT = """ë‹¹ì‹ ì€ í•œêµ­ ìŠ¤íƒ€íŠ¸ì—… ì „ë¬¸ ë¦¬ì„œì²˜ì…ë‹ˆë‹¤.
ëª©í‘œ: 2025ë…„ ê¸°ì¤€, 'í•œêµ­ì˜ AI ê¸°ìˆ ì„ ì‚¬ìš©í•˜ëŠ” í•€í…Œí¬(ê¸ˆìœµ) ìŠ¤íƒ€íŠ¸ì—…'ë§Œ ì¶”ë ¤
'name, domain, description' í•„ë“œë¡œ êµ¬ì„±ëœ JSONì„ ë°˜í™˜í•©ë‹ˆë‹¤.

ê·œì¹™:
- **í•œêµ­ ê¸°ì—…ë§Œ ì„ ì •**: í•œêµ­ì— ë³¸ì‚¬ë¥¼ ë‘” ìŠ¤íƒ€íŠ¸ì—…ë§Œ í¬í•¨. í•´ì™¸ ê¸°ì—…ì€ ì œì™¸.
- ë°˜ë“œì‹œ 'AI ê¸°ìˆ ì„ í•µì‹¬ì— í™œìš©'í•˜ëŠ” í•€í…Œí¬ì¼ ê²ƒ(LLM/RAG/ML/NLP/CV/ì¶”ì²œ/ë¦¬ìŠ¤í¬ëª¨ë¸ ë“±).
- ì€í–‰/ëŒ€ê¸°ì—…ì˜ ì‚¬ì—…ë¶€ë‚˜ BaaS ë²¤ë”ëŠ” ì œì™¸í•˜ê³  'ìŠ¤íƒ€íŠ¸ì—…' ì¤‘ì‹¬.
- íšŒì‚¬ë‹¹ 1~2ë¬¸ì¥ìœ¼ë¡œ descriptionì„ ì‘ì„±(í•œêµ­ì–´).
- domainì€ 'Fintech/ì„¸ë¶€ë¶„ì•¼' í˜•íƒœë¡œ ê°„ê²°í•˜ê²Œ í‘œê¸°(ì˜ˆ: 'Fintech/ì‹ ìš©í‰ê°€', 'Fintech/ê²°ì œ').
- ì¤‘ë³µ/ë™ì¼ íšŒì‚¬ ì œê±°, ìµœëŒ€ {limit}ê°œ.
- ìµœì¢… ì¶œë ¥ì€ JSONë§Œ(ë¬¸ì¥Â·í•´ì„¤ ê¸ˆì§€), ìŠ¤í‚¤ë§ˆ:
  {{"items":[{{"name":"","domain":"","description":""}}, ...]}}
"""

STARTUP_SEARCH_USER_QUERY_TMPL = """ì•„ë˜ëŠ” ì›¹ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤. 2025ë…„ ê¸°ì¤€ìœ¼ë¡œ ìœ íš¨í•œ 'í•œêµ­ AI í•€í…Œí¬ ìŠ¤íƒ€íŠ¸ì—…'ë§Œ ì¶”ë ¤ ì£¼ì„¸ìš”.
í•´ì™¸ ê¸°ì—…ì€ ì œì™¸í•˜ê³ , í•œêµ­ ê¸°ì—…ë§Œ ì„ ì •í•˜ì„¸ìš”.
í•œêµ­ì–´ë¡œ ìš”ì•½í•˜ë©°, ì§€ì • ìŠ¤í‚¤ë§ˆ(JSON-only)ë¡œë§Œ ë‹µí•˜ì„¸ìš”.

ê²€ìƒ‰ ì§ˆì˜:
{query}

ê²€ìƒ‰ ê²°ê³¼(ìµœëŒ€ 30ê°œ):
{results}
"""

TECH_SUMMARY_SYSTEM_PROMPT = """ë‹¹ì‹ ì€ í•€í…Œí¬ ì›ì²œ ê¸°ìˆ  ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**ë¶„ì„ ëª©í‘œ: êµ¬ì²´ì ì¸ AI ì›ì²œ ê¸°ìˆ  íŒŒì•…**

í•µì‹¬ ì›ì¹™:
1. **êµ¬ì²´ì ì¸ ê¸°ìˆ ë§Œ ì¶”ì¶œ**: "AI í™œìš©" ê°™ì€ ì¼ë°˜ì  í‘œí˜„ ê¸ˆì§€
2. **ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜**: ëª…ì‹œëœ ë‚´ìš©ë§Œ ì‚¬ìš©, ì¶”ì¸¡ ê¸ˆì§€
3. **ê¸°ìˆ  ìŠ¤íƒ ìš°ì„ **: ì•Œê³ ë¦¬ì¦˜, ëª¨ë¸, í”„ë ˆì„ì›Œí¬ ë“± êµ¬ì²´ì  ê¸°ìˆ  ì •ë³´
4. **ì •ë³´ ë¶€ì¡± ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸**: ì¥ì ì´ë‚˜ í•œê³„ì  ì •ë³´ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ [] ë°˜í™˜

ë¶„ì„ ì‘ì—…:

1. **ì œí’ˆ/ì„œë¹„ìŠ¤ ì‹ë³„**
   - ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ëª…í™•í•œ ì œí’ˆ/ì„œë¹„ìŠ¤ë§Œ ì¶”ì¶œ
   - ì—¬ëŸ¬ ì œí’ˆì´ ìˆìœ¼ë©´ ê°ê° ë³„ë„ ê°ì²´ë¡œ ìƒì„±

2. **ì›ì²œ ê¸°ìˆ  ì„¤ëª… (description) - ê°€ì¥ ì¤‘ìš”**
   ë‹¤ìŒ ì •ë³´ë¥¼ ìš°ì„  ìˆœìœ„ëŒ€ë¡œ ì¶”ì¶œ:
   
   a) **AI/ML ì•Œê³ ë¦¬ì¦˜**: 
      - ì˜ˆ: "Random Forest ê¸°ë°˜ ì‹ ìš©í‰ê°€", "LSTM ì‹œê³„ì—´ ì˜ˆì¸¡", "Transformer ê¸°ë°˜ NLP"
      - ì˜ˆ: "XGBoost ë¦¬ìŠ¤í¬ ëª¨ë¸", "ê°•í™”í•™ìŠµ ì¶”ì²œ ì‹œìŠ¤í…œ"
      
   b) **êµ¬ì²´ì  ëª¨ë¸**:
      - ì˜ˆ: "BERT íŒŒì¸íŠœë‹", "GPT ê¸°ë°˜ ì±—ë´‡", "CNN ë¬¸ì„œ ë¶„ì„"
      
   c) **ê¸°ìˆ  í”„ë ˆì„ì›Œí¬/ë¼ì´ë¸ŒëŸ¬ë¦¬**:
      - ì˜ˆ: "TensorFlow", "PyTorch", "scikit-learn", "Keras"
      
   d) **ë°ì´í„° ì²˜ë¦¬ ê¸°ìˆ **:
      - ì˜ˆ: "ë¹„ì •í˜• ë°ì´í„° ì²˜ë¦¬", "ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬", "ëŒ€ê·œëª¨ ë¶„ì‚° ì²˜ë¦¬"
      
   e) **íŠ¹í™” ê¸°ìˆ **:
      - ì˜ˆ: "ìì—°ì–´ì²˜ë¦¬(NLP)", "ì»´í“¨í„° ë¹„ì „(CV)", "ì¶”ì²œ ì—”ì§„", "ì´ìƒ íƒì§€"

   **ì‘ì„± í˜•ì‹**:
   "[ì„œë¹„ìŠ¤ëª…]: [êµ¬ì²´ì  AI ê¸°ìˆ ] ê¸°ë°˜ì˜ [ê¸ˆìœµ ì„œë¹„ìŠ¤]. [ì¶”ê°€ ê¸°ìˆ  ìƒì„¸]"
   
   **ì¢‹ì€ ì˜ˆì‹œ**:
   - "AI ì‹ ìš©í‰ê°€: Gradient Boosting(XGBoost) ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜ ëŒ€ì•ˆì‹ ìš©í‰ê°€ ëª¨ë¸. ë¹„ì •í˜• ë°ì´í„°(í†µì‹ , ê²°ì œ ë“±) í•™ìŠµ"
   - "ì±—ë´‡: BERT íŒŒì¸íŠœë‹í•œ ê¸ˆìœµ ìƒë‹´ NLP ëª¨ë¸. ì˜ë„ ë¶„ë¥˜ ë° ì—”í‹°í‹° ì¶”ì¶œ"
   
   **ë‚˜ìœ ì˜ˆì‹œ**:
   - "AI ê¸°ìˆ ì„ í™œìš©í•œ ì‹ ìš©í‰ê°€" âŒ
   - "ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì„œë¹„ìŠ¤" âŒ

   **ì •ë³´ ë¶€ì¡±ì‹œ**: "í•€í…Œí¬ [ë¶„ì•¼] ì„œë¹„ìŠ¤ (êµ¬ì²´ì  ê¸°ìˆ  ì •ë³´ ë¶€ì¡±)"

3. **ì¥ì  (strengths)**
   - ê¸°ìˆ ì  ì°¨ë³„ì , ì„±ê³¼, ì •í™•ë„ ê°œì„  ë“±
   - êµ¬ì²´ì  ìˆ˜ì¹˜ í¬í•¨ (ì˜ˆ: "ì •í™•ë„ 15% í–¥ìƒ")
   - ìµœì†Œ 2ê°œ, ìµœëŒ€ 5ê°œ
   - **ì •ë³´ ì—†ìœ¼ë©´: ë¹ˆ ë¦¬ìŠ¤íŠ¸ []**

4. **í•œê³„ì  (limitations)**
   - ê¸°ìˆ ì  ì œì•½, ê°œì„  ê³¼ì œ
   - **ì •ë³´ ì—†ìœ¼ë©´: ë¹ˆ ë¦¬ìŠ¤íŠ¸ []**

ì¶œë ¥ ìŠ¤í‚¤ë§ˆ (JSONë§Œ):
{{
  "company": {{"name": "", "domain": "", "description": ""}},
  "products": [
    {{
      "name": "",
      "description": "",
      "strengths": [],
      "limitations": []
    }}
  ]
}}
"""

TECH_SUMMARY_USER_QUERY_TMPL = """ë‹¤ìŒ íšŒì‚¬ì˜ ì›ì²œ ê¸°ìˆ  ì •ë³´ë¥¼ ë¶„ì„í•˜ì„¸ìš”.

íšŒì‚¬ ì •ë³´:
- ì´ë¦„: {company_name}
- ë¶„ì•¼: {company_domain}  
- ê°œìš”: {company_description}

=== ìˆ˜ì§‘ëœ ê¸°ìˆ  ì •ë³´ ===
{search_results}
========================

**ë¶„ì„ ì§€ì¹¨:**
1. êµ¬ì²´ì ì¸ AI/ML ì•Œê³ ë¦¬ì¦˜, ëª¨ë¸, í”„ë ˆì„ì›Œí¬ë¥¼ ì°¾ìœ¼ì„¸ìš”
2. "AI í™œìš©", "ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜" ê°™ì€ ì¼ë°˜ì  í‘œí˜„ì€ í”¼í•˜ì„¸ìš”
3. ê²€ìƒ‰ ê²°ê³¼ì— ëª…ì‹œëœ ê¸°ìˆ ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
4. ì¥ì /í•œê³„ì  ì •ë³´ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ []ë¡œ ë°˜í™˜í•˜ì„¸ìš”

JSONìœ¼ë¡œë§Œ ë°˜í™˜í•˜ì„¸ìš”.
"""


# ============================================================================
# LLM ì´ˆê¸°í™”
# ============================================================================
_search_llm = ChatOpenAI(model=OPENAI_MODEL, temperature=0.1)
_summary_llm = ChatOpenAI(model=OPENAI_MODEL, temperature=0.0)


# ============================================================================
# ë…¸ë“œ 1: ìŠ¤íƒ€íŠ¸ì—… ê²€ìƒ‰
# ============================================================================
def startup_search_node(state: AgentState) -> AgentState:
    """ìŠ¤íƒ€íŠ¸ì—… ê²€ìƒ‰ ë…¸ë“œ"""
    print(f"\n{'=' * 70}")
    print(f"ğŸ“ 1ë‹¨ê³„: ìŠ¤íƒ€íŠ¸ì—… ê²€ìƒ‰ (ìµœëŒ€ {state.limit}ê°œ)")
    print("=" * 70)
    
    state.current_step = "startup_search"
    
    base_terms = [
        "í•œêµ­ AI ì‹ ìš©í‰ê°€ ìŠ¤íƒ€íŠ¸ì—…",
        "êµ­ë‚´ ë¡œë³´ì–´ë“œë°”ì´ì € ìŠ¤íƒ€íŠ¸ì—…",
        "í•œêµ­ AI ëŒ€ì¶œ í•€í…Œí¬",
        "êµ­ë‚´ ì´ìƒê±°ë˜íƒì§€ FDS AI",
        "í•œêµ­ í•€í…Œí¬ ìŠ¤íƒ€íŠ¸ì—… íˆ¬ììœ ì¹˜",
        "êµ­ë‚´ ê¸ˆìœµ AI ìŠ¤íƒ€íŠ¸ì—… ì‹œë¦¬ì¦ˆ"
    ]
    
    if state.region.lower() == "korea":
        base_terms += [
            "í¬ë ˆíŒŒìŠ¤ ë±…í¬ìƒëŸ¬ë“œ í† ìŠ¤",
            "í•€í…Œí¬ ìŠ¤íƒ€íŠ¸ì—… AI ê¸°ìˆ  í™œìš©"
        ]

    query = " OR ".join(base_terms)
    search_results = web_search(query)

    sys = STARTUP_SEARCH_SYSTEM_PROMPT.format(limit=state.limit)
    user = STARTUP_SEARCH_USER_QUERY_TMPL.format(query=query, results=search_results)

    try:
        structured = _search_llm.with_structured_output(StartupSearchResult)
        out: StartupSearchResult = structured.invoke([
            SystemMessage(content=sys),
            HumanMessage(content=user)
        ])

        uniq = {}
        cleaned: List[StartupHit] = []
        for item in out.items:
            key = item.name.strip().lower()
            if key in uniq:
                continue
            uniq[key] = True
            cleaned.append(StartupHit(
                name=item.name.strip(),
                domain=item.domain.strip(),
                description=item.description.strip()
            ))
        
        state.startups = cleaned[:state.limit]
        
        print(f"\nâœ… {len(state.startups)}ê°œ ìŠ¤íƒ€íŠ¸ì—… ë°œê²¬")
        for i, s in enumerate(state.startups, 1):
            print(f"   {i}. {s.name} ({s.domain})")
        
    except Exception as e:
        state.error = f"ìŠ¤íƒ€íŠ¸ì—… ê²€ìƒ‰ ì‹¤íŒ¨: {e}"
        print(f"âŒ {state.error}")
    
    return state


# ============================================================================
# ë…¸ë“œ 2: ê¸°ìˆ  ìš”ì•½
# ============================================================================
def collect_company_tech_info(company_name: str, company_domain: str) -> str:
    """íšŒì‚¬ì— ëŒ€í•œ ê¸°ìˆ  ì •ë³´ë¥¼ ì›¹ì—ì„œ ìˆ˜ì§‘"""
    collected_info = []
    
    search_queries = [
        f"{company_name} ê¸°ìˆ  ìŠ¤íƒ architecture AI",
        f"{company_name} AI ëª¨ë¸ ì•Œê³ ë¦¬ì¦˜ ë¨¸ì‹ ëŸ¬ë‹",
        f"{company_name} ê°œë°œ ê¸°ìˆ  ë¸”ë¡œê·¸ tech",
        f"{company_name} engineering blog",
        f"{company_name} íŠ¹í—ˆ ë…¼ë¬¸ patent"
    ]
    
    print(f"   ğŸ” ê¸°ìˆ  ì •ë³´ ê²€ìƒ‰ ì¤‘...")
    
    seen_urls = set()
    collected_count = 0
    
    for query in search_queries[:4]:
        results = web_search(query)
        
        if not isinstance(results, list) or len(results) == 0:
            continue
        
        for result in results[:6]:
            if not isinstance(result, dict):
                continue
                
            title = result.get('title', '').lower()
            content = result.get('content', '')
            url = result.get('url', '')
            
            if url in seen_urls:
                continue
            
            tech_keywords = [
                'ai', 'ml', 'machine learning', 'deep learning', 'nlp', 
                'algorithm', 'model', 'ì•Œê³ ë¦¬ì¦˜', 'ëª¨ë¸', 'ë¨¸ì‹ ëŸ¬ë‹', 
                'ë”¥ëŸ¬ë‹', 'ì¸ê³µì§€ëŠ¥', 'tensorflow', 'pytorch', 'bert', 
                'gpt', 'transformer', 'lstm', 'xgboost', 'ê¸°ìˆ ', 'tech'
            ]
            
            has_company = company_name.lower() in title or company_name.lower() in content.lower()
            has_tech = any(keyword in title or keyword in content.lower() for keyword in tech_keywords)
            
            if has_company and has_tech and content and len(content) > 100:
                seen_urls.add(url)
                collected_info.append(
                    f"[ì¶œì²˜: {result.get('title', 'Unknown')}]\n{content}\n"
                )
                collected_count += 1
                
                if collected_count >= 6:
                    break
        
        if collected_count >= 6:
            break
    
    if not collected_info:
        return f"{company_name}ì— ëŒ€í•œ ê¸°ìˆ  ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    print(f"   âœ… {collected_count}ê°œ ì¶œì²˜ì—ì„œ ì •ë³´ ìˆ˜ì§‘")
    return "\n\n".join(collected_info)


def tech_summary_node(state: AgentState) -> AgentState:
    """ê¸°ìˆ  ìš”ì•½ ë…¸ë“œ"""
    print(f"\n{'=' * 70}")
    print(f"ğŸ—œï¸  2ë‹¨ê³„: ê¸°ìˆ  ìš”ì•½ (ì´ {len(state.startups)}ê°œ íšŒì‚¬)")
    print("=" * 70)
    
    state.current_step = "tech_summary"
    
    if not state.startups:
        state.error = "ê²€ìƒ‰ëœ ìŠ¤íƒ€íŠ¸ì—…ì´ ì—†ìŠµë‹ˆë‹¤"
        print(f"âŒ {state.error}")
        return state
    
    company_details = []
    
    for startup in state.startups:
        print(f"\nğŸ”¬ [{startup.name}] ì›ì²œ ê¸°ìˆ  ë¶„ì„ ì‹œì‘...")
        
        # íšŒì‚¬ ì •ë³´ ìˆ˜ì§‘
        search_results = collect_company_tech_info(startup.name, startup.domain)
        
        if "ê¸°ìˆ  ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in search_results:
            print(f"   âš ï¸  ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            company_details.append(CompanyDetail(
                company=Company(
                    name=startup.name,
                    domain=startup.domain,
                    description=startup.description
                ),
                products=[]
            ))
            continue
        
        # LLM ë¶„ì„
        sys = TECH_SUMMARY_SYSTEM_PROMPT
        user = TECH_SUMMARY_USER_QUERY_TMPL.format(
            company_name=startup.name,
            company_domain=startup.domain,
            company_description=startup.description,
            search_results=search_results
        )
        
        try:
            structured = _summary_llm.with_structured_output(CompanyDetail)
            detail: CompanyDetail = structured.invoke([
                SystemMessage(content=sys),
                HumanMessage(content=user)
            ])
            
            if detail.products:
                print(f"   âœ… ë¶„ì„ ì™„ë£Œ: ì œí’ˆ {len(detail.products)}ê°œ")
                for prod in detail.products:
                    print(f"      - {prod.name}: {prod.description[:70]}...")
            else:
                print(f"   âš ï¸  ì œí’ˆ ì •ë³´ ì—†ìŒ")
            
            company_details.append(detail)
            
        except Exception as e:
            print(f"   âŒ LLM ë¶„ì„ ì‹¤íŒ¨: {e}")
            company_details.append(CompanyDetail(
                company=Company(
                    name=startup.name,
                    domain=startup.domain,
                    description=startup.description
                ),
                products=[]
            ))
    
    state.company_details = company_details
    return state


# ============================================================================
# ë…¸ë“œ 3: ê²°ê³¼ ì¶œë ¥ (ë‹¤ìŒ ì—ì´ì „íŠ¸ê°€ ì¶”ê°€ë˜ë©´ ì´ ë…¸ë“œ ëŒ€ì‹  ë‹¤ìŒ ì—ì´ì „íŠ¸ë¡œ)
# ============================================================================
def output_node(state: AgentState) -> AgentState:
    """ê²°ê³¼ ì¶œë ¥ ë…¸ë“œ (ë‚˜ì¤‘ì— ë‹¤ìŒ ì—ì´ì „íŠ¸ ë…¸ë“œë¡œ ëŒ€ì²´ë  ì˜ˆì •)"""
    print(f"\n{'=' * 70}")
    print("ğŸ“Š ìµœì¢… ê²°ê³¼")
    print("=" * 70)
    
    state.current_step = "output"
    
    # í†µê³„
    total_products = sum(len(d.products) for d in state.company_details)
    companies_with_products = sum(1 for d in state.company_details if d.products)
    
    print(f"\nâœ… ë¶„ì„ ì™„ë£Œ!")
    print(f"   - ì´ íšŒì‚¬: {len(state.company_details)}ê°œ")
    print(f"   - ì œí’ˆ ì •ë³´ í™•ì¸: {companies_with_products}ê°œ")
    print(f"   - ì´ ì œí’ˆ/ì„œë¹„ìŠ¤: {total_products}ê°œ")
    
    # JSON ì¶œë ¥
    output = {
        "total_companies": len(state.company_details),
        "companies": [detail.model_dump() for detail in state.company_details]
    }
    
    print(f"\n{json.dumps(output, ensure_ascii=False, indent=2)}")
    
    # íŒŒì¼ ì €ì¥
    output_file = "tech_summary_result.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ ê²°ê³¼ê°€ '{output_file}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    return state


# ============================================================================
# ê·¸ë˜í”„ ë¹Œë“œ
# ============================================================================
def build_graph():
    """LangGraph ë¹Œë“œ"""
    workflow = StateGraph(AgentState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("startup_search", startup_search_node)
    workflow.add_node("tech_summary", tech_summary_node)
    workflow.add_node("output", output_node)
    # ë‚˜ì¤‘ì— ì¶”ê°€ë  ë…¸ë“œë“¤:
    # workflow.add_node("market_evaluation", market_evaluation_node)
    # workflow.add_node("investment_decision", investment_decision_node)
    
    # ì—£ì§€ ì •ì˜
    workflow.set_entry_point("startup_search")
    workflow.add_edge("startup_search", "tech_summary")
    workflow.add_edge("tech_summary", "output")
    # ë‚˜ì¤‘ì— ì¶”ê°€ë  ì—£ì§€ë“¤:
    # workflow.add_edge("tech_summary", "market_evaluation")
    # workflow.add_edge("market_evaluation", "investment_decision")
    workflow.add_edge("output", END)
    
    return workflow.compile(checkpointer=MemorySaver())


# ============================================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================================
def main():
    p = argparse.ArgumentParser(description="AI Fintech Tech Summary Agent - LangGraph")
    p.add_argument("--region", default="Korea", help="Global | Korea")
    p.add_argument("--limit", type=int, default=5, help="ë¶„ì„í•  ìŠ¤íƒ€íŠ¸ì—… ìˆ˜")
    args = p.parse_args()

    print("=" * 70)
    print("ğŸš€ AI í•€í…Œí¬ ì›ì²œ ê¸°ìˆ  ë¶„ì„ íŒŒì´í”„ë¼ì¸ (LangGraph)")
    print("=" * 70)
    
    # ê·¸ë˜í”„ ë¹Œë“œ
    app = build_graph()
    
    # ì´ˆê¸° ìƒíƒœ
    initial_state = AgentState(
        region=args.region,
        limit=args.limit
    )
    
    # ê·¸ë˜í”„ ì‹¤í–‰
    config = {"configurable": {"thread_id": "tech_analysis_pipeline"}}
    final_state = app.invoke(initial_state, config)
    
    print(f"\n{'=' * 70}")
    print("âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
    print(f"ìµœì¢… ìƒíƒœ: {final_state.current_step}")
    if final_state.error:
        print(f"ì—ëŸ¬: {final_state.error}")
    print("=" * 70)
    
    # ë‹¤ìŒ ì—ì´ì „íŠ¸ë¡œ ì „ë‹¬í•  ë°ì´í„° (company_details)
    print(f"\nğŸ’¡ ë‹¤ìŒ ì—ì´ì „íŠ¸ì—ê²Œ ì „ë‹¬ë  ë°ì´í„°:")
    print(f"   - company_details: {len(final_state.company_details)}ê°œ íšŒì‚¬ ìƒì„¸ ì •ë³´")
    print(f"   - ê° íšŒì‚¬ëŠ” Company + List[Product] êµ¬ì¡°")
    
    return final_state


if __name__ == "__main__":
    main()